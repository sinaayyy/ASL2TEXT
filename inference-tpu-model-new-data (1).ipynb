{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ad1425a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-26T09:32:33.824025Z",
     "iopub.status.busy": "2023-07-26T09:32:33.823341Z",
     "iopub.status.idle": "2023-07-26T09:32:33.82896Z",
     "shell.execute_reply": "2023-07-26T09:32:33.8281Z",
     "shell.execute_reply.started": "2023-07-26T09:32:33.823944Z"
    },
    "papermill": {
     "duration": 0.016574,
     "end_time": "2023-08-25T06:05:31.360681",
     "exception": false,
     "start_time": "2023-08-25T06:05:31.344107",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bd3dd78",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-08-25T06:05:31.393213Z",
     "iopub.status.busy": "2023-08-25T06:05:31.392865Z",
     "iopub.status.idle": "2023-08-25T06:05:40.153845Z",
     "shell.execute_reply": "2023-08-25T06:05:40.152597Z"
    },
    "papermill": {
     "duration": 8.78238,
     "end_time": "2023-08-25T06:05:40.157685",
     "exception": false,
     "start_time": "2023-08-25T06:05:31.375305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version 2.12.0\n",
      "Python Version: 3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:40:32) [GCC 12.3.0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sn\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from leven import levenshtein\n",
    "\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import gc\n",
    "import sys\n",
    "import sklearn\n",
    "import time\n",
    "import json\n",
    "\n",
    "# TQDM Progress Bar With Pandas Apply Function\n",
    "tqdm.pandas()\n",
    "\n",
    "print(f'Tensorflow Version {tf.__version__}')\n",
    "print(f'Python Version: {sys.version}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0896d790",
   "metadata": {
    "papermill": {
     "duration": 0.01066,
     "end_time": "2023-08-25T06:05:40.184702",
     "exception": false,
     "start_time": "2023-08-25T06:05:40.174042",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de761738",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T06:05:40.208075Z",
     "iopub.status.busy": "2023-08-25T06:05:40.207401Z",
     "iopub.status.idle": "2023-08-25T06:05:40.213586Z",
     "shell.execute_reply": "2023-08-25T06:05:40.212528Z"
    },
    "papermill": {
     "duration": 0.020482,
     "end_time": "2023-08-25T06:05:40.215976",
     "exception": false,
     "start_time": "2023-08-25T06:05:40.195494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "SEED = 42\n",
    "def seed_everything(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "998b2ae6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T06:05:40.238016Z",
     "iopub.status.busy": "2023-08-25T06:05:40.237692Z",
     "iopub.status.idle": "2023-08-25T06:05:40.243950Z",
     "shell.execute_reply": "2023-08-25T06:05:40.242943Z"
    },
    "papermill": {
     "duration": 0.019911,
     "end_time": "2023-08-25T06:05:40.246124",
     "exception": false,
     "start_time": "2023-08-25T06:05:40.226213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If Notebook Is Run By Committing or In Interactive Mode For Development\n",
    "IS_INTERACTIVE = os.environ['KAGGLE_KERNEL_RUN_TYPE'] == 'Interactive'\n",
    "# Verbose Setting during training\n",
    "VERBOSE = 1 if IS_INTERACTIVE else 2\n",
    "# Global Random Seed\n",
    "SEED = 42\n",
    "# Number of Frames to resize recording to\n",
    "FRAME_LEN = 128\n",
    "# Number of colonne\n",
    "N_COLS = 164\n",
    "# Length of Phrase + EOS Token\n",
    "MAX_PHRASE_LENGTH = 64\n",
    "# Global debug flag, takes subset of train\n",
    "DEBUG = False\n",
    "# Whether to use 10% of data for validation\n",
    "USE_VAL = False\n",
    "# Batch Size\n",
    "BATCH_SIZE = 1024\n",
    "# Whether to Load Pretrained Weights\n",
    "LOAD_WEIGHTS = True\n",
    "#PAD \n",
    "pad_token = '^'\n",
    "pad_token_idx = 59\n",
    "#Input\n",
    "INPUT_SHAPE = [128, 276]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e5d92a",
   "metadata": {
    "papermill": {
     "duration": 0.010189,
     "end_time": "2023-08-25T06:05:40.266449",
     "exception": false,
     "start_time": "2023-08-25T06:05:40.256260",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ecba5f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T06:05:40.289524Z",
     "iopub.status.busy": "2023-08-25T06:05:40.289243Z",
     "iopub.status.idle": "2023-08-25T06:05:40.499856Z",
     "shell.execute_reply": "2023-08-25T06:05:40.498566Z"
    },
    "papermill": {
     "duration": 0.225702,
     "end_time": "2023-08-25T06:05:40.502747",
     "exception": false,
     "start_time": "2023-08-25T06:05:40.277045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n",
    "    char_to_num = json.load(f)\n",
    "\n",
    "pad_token = '^'\n",
    "pad_token_idx = 59\n",
    "\n",
    "char_to_num[pad_token] = pad_token_idx\n",
    "\n",
    "num_to_char = {j:i for i,j in char_to_num.items()}\n",
    "df = pd.read_csv('/kaggle/input/asl-fingerspelling/train.csv')\n",
    "\n",
    "LIP = [\n",
    "    61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "    291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "    78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "    95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
    "]\n",
    "LPOSE = [13, 15, 17, 19, 21]\n",
    "RPOSE = [14, 16, 18, 20, 22]\n",
    "POSE = LPOSE + RPOSE\n",
    "\n",
    "X = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE] + [f'x_face_{i}' for i in LIP]\n",
    "Y = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_pose_{i}' for i in POSE] + [f'y_face_{i}' for i in LIP]\n",
    "Z = [f'z_right_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)] + [f'z_pose_{i}' for i in POSE] + [f'z_face_{i}' for i in LIP]\n",
    "\n",
    "SEL_COLS = X + Y + Z\n",
    "FRAME_LEN = 128\n",
    "MAX_PHRASE_LENGTH = 64\n",
    "\n",
    "LIP_IDX_X   = [i for i, col in enumerate(SEL_COLS)  if  \"face\" in col and \"x\" in col]\n",
    "RHAND_IDX_X = [i for i, col in enumerate(SEL_COLS)  if \"right\" in col and \"x\" in col]\n",
    "LHAND_IDX_X = [i for i, col in enumerate(SEL_COLS)  if  \"left\" in col and \"x\" in col]\n",
    "RPOSE_IDX_X = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in RPOSE and \"x\" in col]\n",
    "LPOSE_IDX_X = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in LPOSE and \"x\" in col]\n",
    "\n",
    "LIP_IDX_Y   = [i for i, col in enumerate(SEL_COLS)  if  \"face\" in col and \"y\" in col]\n",
    "RHAND_IDX_Y = [i for i, col in enumerate(SEL_COLS)  if \"right\" in col and \"y\" in col]\n",
    "LHAND_IDX_Y = [i for i, col in enumerate(SEL_COLS)  if  \"left\" in col and \"y\" in col]\n",
    "RPOSE_IDX_Y = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in RPOSE and \"y\" in col]\n",
    "LPOSE_IDX_Y = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in LPOSE and \"y\" in col]\n",
    "\n",
    "LIP_IDX_Z   = [i for i, col in enumerate(SEL_COLS)  if  \"face\" in col and \"z\" in col]\n",
    "RHAND_IDX_Z = [i for i, col in enumerate(SEL_COLS)  if \"right\" in col and \"z\" in col]\n",
    "LHAND_IDX_Z = [i for i, col in enumerate(SEL_COLS)  if  \"left\" in col and \"z\" in col]\n",
    "RPOSE_IDX_Z = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in RPOSE and \"z\" in col]\n",
    "LPOSE_IDX_Z = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in LPOSE and \"z\" in col]\n",
    "\n",
    "RHM = np.load(\"/kaggle/input/personnal-data-3/mean_std/rh_mean.npy\")\n",
    "LHM = np.load(\"/kaggle/input/personnal-data-3/mean_std/lh_mean.npy\")\n",
    "RPM = np.load(\"/kaggle/input/personnal-data-3/mean_std/rp_mean.npy\")\n",
    "LPM = np.load(\"/kaggle/input/personnal-data-3/mean_std/lp_mean.npy\")\n",
    "LIPM = np.load(\"/kaggle/input/personnal-data-3/mean_std/lip_mean.npy\")\n",
    "\n",
    "RHS = np.load(\"/kaggle/input/personnal-data-3/mean_std/rh_std.npy\")\n",
    "LHS = np.load(\"/kaggle/input/personnal-data-3/mean_std/lh_std.npy\")\n",
    "RPS = np.load(\"/kaggle/input/personnal-data-3/mean_std/rp_std.npy\")\n",
    "LPS = np.load(\"/kaggle/input/personnal-data-3/mean_std/lp_std.npy\")\n",
    "LIPS = np.load(\"/kaggle/input/personnal-data-3/mean_std/lip_std.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee17c9ee",
   "metadata": {
    "papermill": {
     "duration": 0.010846,
     "end_time": "2023-08-25T06:05:40.524794",
     "exception": false,
     "start_time": "2023-08-25T06:05:40.513948",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "370d5a17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T06:05:40.547144Z",
     "iopub.status.busy": "2023-08-25T06:05:40.546751Z",
     "iopub.status.idle": "2023-08-25T06:05:40.558964Z",
     "shell.execute_reply": "2023-08-25T06:05:40.558083Z"
    },
    "papermill": {
     "duration": 0.025694,
     "end_time": "2023-08-25T06:05:40.561160",
     "exception": false,
     "start_time": "2023-08-25T06:05:40.535466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_len: 3\n"
     ]
    }
   ],
   "source": [
    "tffiles = [f\"/kaggle/input/personnal-data-3/tfds/{file_id}.tfrecord\" for file_id in df.file_id.unique()]\n",
    "val_len = int(0.05 * len(tffiles))\n",
    "print('val_len: ' + str(val_len))\n",
    "train_batch_size = 1024\n",
    "val_batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa225465",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T06:05:40.583195Z",
     "iopub.status.busy": "2023-08-25T06:05:40.582918Z",
     "iopub.status.idle": "2023-08-25T06:05:43.559113Z",
     "shell.execute_reply": "2023-08-25T06:05:43.557787Z"
    },
    "papermill": {
     "duration": 2.990142,
     "end_time": "2023-08-25T06:05:43.561667",
     "exception": false,
     "start_time": "2023-08-25T06:05:40.571525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_relevant_data_subset(pq_path):\n",
    "    return pd.read_parquet(pq_path, columns=SEL_COLS)\n",
    "\n",
    "file_id = df.file_id.iloc[0]\n",
    "inpdir = \"/kaggle/input/asl-fingerspelling/train_landmarks\"\n",
    "pqfile = f\"{inpdir}/{file_id}.parquet\"\n",
    "seq_refs = df.loc[df.file_id == file_id]\n",
    "seqs = load_relevant_data_subset(pqfile)\n",
    "\n",
    "seq_id = seq_refs.sequence_id.iloc[0]\n",
    "frames = seqs.iloc[seqs.index == seq_id]\n",
    "phrase = str(df.loc[df.sequence_id == seq_id].phrase.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b82c90d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T06:05:43.585584Z",
     "iopub.status.busy": "2023-08-25T06:05:43.584857Z",
     "iopub.status.idle": "2023-08-25T06:05:50.196637Z",
     "shell.execute_reply": "2023-08-25T06:05:50.195500Z"
    },
    "papermill": {
     "duration": 6.626394,
     "end_time": "2023-08-25T06:05:50.199076",
     "exception": false,
     "start_time": "2023-08-25T06:05:43.572682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128, 276]\n"
     ]
    }
   ],
   "source": [
    "@tf.function()\n",
    "def tf_nan_mean(x, axis=0, keepdims=False):\n",
    "\n",
    "    return tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), x), axis=axis, keepdims=keepdims) / tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), tf.ones_like(x)), axis=axis, keepdims=keepdims)\n",
    "\n",
    " \n",
    "@tf.function()\n",
    "def tf_nan_std(x, center=None, axis=0, keepdims=False):\n",
    "\n",
    "    if center is None:\n",
    "\n",
    "        center = tf_nan_mean(x, axis=axis,  keepdims=True)\n",
    "\n",
    "    d = x - center\n",
    "\n",
    "    return tf.math.sqrt(tf_nan_mean(d * d, axis=axis, keepdims=keepdims))\n",
    "\n",
    "@tf.function()\n",
    "def resize_pad(x):\n",
    "    if tf.shape(x)[0] < FRAME_LEN:\n",
    "        x = tf.pad(x, ([[0, FRAME_LEN-tf.shape(x)[0]], [0, 0], [0, 0]]), constant_values=float(-1000.0))\n",
    "    else:\n",
    "        x = tf.image.resize(x, (FRAME_LEN, tf.shape(x)[1]))\n",
    "    return x\n",
    "\n",
    "@tf.function(jit_compile=True)\n",
    "def pre_process0(x):\n",
    "    lip_x = tf.gather(x, LIP_IDX_X, axis=1)\n",
    "    lip_y = tf.gather(x, LIP_IDX_Y, axis=1)\n",
    "    lip_z = tf.gather(x, LIP_IDX_Z, axis=1)\n",
    "\n",
    "    rhand_x = tf.gather(x, RHAND_IDX_X, axis=1)\n",
    "    rhand_y = tf.gather(x, RHAND_IDX_Y, axis=1)\n",
    "    rhand_z = tf.gather(x, RHAND_IDX_Z, axis=1)\n",
    "    \n",
    "    lhand_x = tf.gather(x, LHAND_IDX_X, axis=1)\n",
    "    lhand_y = tf.gather(x, LHAND_IDX_Y, axis=1)\n",
    "    lhand_z = tf.gather(x, LHAND_IDX_Z, axis=1)\n",
    "\n",
    "    rpose_x = tf.gather(x, RPOSE_IDX_X, axis=1)\n",
    "    rpose_y = tf.gather(x, RPOSE_IDX_Y, axis=1)\n",
    "    rpose_z = tf.gather(x, RPOSE_IDX_Z, axis=1)\n",
    "    \n",
    "    lpose_x = tf.gather(x, LPOSE_IDX_X, axis=1)\n",
    "    lpose_y = tf.gather(x, LPOSE_IDX_Y, axis=1)\n",
    "    lpose_z = tf.gather(x, LPOSE_IDX_Z, axis=1)\n",
    "    \n",
    "    lip   = tf.concat([lip_x[..., tf.newaxis], lip_y[..., tf.newaxis], lip_z[..., tf.newaxis]], axis=-1)\n",
    "    rhand = tf.concat([rhand_x[..., tf.newaxis], rhand_y[..., tf.newaxis], rhand_z[..., tf.newaxis]], axis=-1)\n",
    "    lhand = tf.concat([lhand_x[..., tf.newaxis], lhand_y[..., tf.newaxis], lhand_z[..., tf.newaxis]], axis=-1)\n",
    "    rpose = tf.concat([rpose_x[..., tf.newaxis], rpose_y[..., tf.newaxis], rpose_z[..., tf.newaxis]], axis=-1)\n",
    "    lpose = tf.concat([lpose_x[..., tf.newaxis], lpose_y[..., tf.newaxis], lpose_z[..., tf.newaxis]], axis=-1)\n",
    "#     hand = tf.concat([rhand, lhand], axis=1)\n",
    "#     hand = tf.where(tf.math.is_nan(hand), 0.0, hand)\n",
    "#     mask = tf.math.not_equal(tf.reduce_sum(hand, axis=[1, 2]), 0.0)\n",
    "\n",
    "#     lip = lip[mask]\n",
    "#     rhand = rhand[mask]\n",
    "#     lhand = lhand[mask]\n",
    "#     rpose = rpose[mask]\n",
    "#     lpose = lpose[mask]\n",
    "    lip = tf.where(tf.math.is_nan(lip), 0.0, lip)\n",
    "    rhand = tf.where(tf.math.is_nan(rhand), 0.0, rhand)\n",
    "    lhand = tf.where(tf.math.is_nan(lhand), 0.0, lhand)\n",
    "    rpose = tf.where(tf.math.is_nan(rpose), 0.0, rpose)\n",
    "    lpose = tf.where(tf.math.is_nan(lpose), 0.0, lpose)\n",
    "\n",
    "    return lip, rhand, lhand, rpose, lpose\n",
    "\n",
    "@tf.function()\n",
    "# def pre_process1(lip, rhand, lhand, rpose, lpose):\n",
    "#     lip   = (resize_pad(lip) - LIPM) / LIPS\n",
    "#     rhand = (resize_pad(rhand) - RHM) / RHS\n",
    "#     lhand = (resize_pad(lhand) - LHM) / LHS\n",
    "#     rpose = (resize_pad(rpose) - RPM) / RPS\n",
    "#     lpose = (resize_pad(lpose) - LPM) / LPS\n",
    "\n",
    "#     x = tf.concat([lip, rhand, lhand, rpose, lpose], axis=1)\n",
    "#     s = tf.shape(x)\n",
    "#     x = tf.reshape(x, (s[0], s[1]*s[2]))\n",
    "#     x = tf.where(tf.math.is_nan(x), 0.0, x)\n",
    "#     return x\n",
    "\n",
    "def pre_process1(lip, rhand, lhand, rpose, lpose):\n",
    "    lip   = (lip - tf_nan_mean(lip, keepdims=True)) / (tf_nan_std(lip, keepdims=True))\n",
    "    rhand = (rhand - tf_nan_mean(rhand, keepdims=True)) / (tf_nan_std(rhand, keepdims=True))\n",
    "    lhand = (lhand - tf_nan_mean(lhand, keepdims=True)) / (tf_nan_std(lhand, keepdims=True))\n",
    "    rpose = (rpose - tf_nan_mean(rpose, keepdims=True)) / (tf_nan_std(rpose, keepdims=True))\n",
    "    lpose = (lpose - tf_nan_mean(lpose, keepdims=True)) / (tf_nan_std(lpose, keepdims=True))\n",
    "\n",
    "    x = tf.concat([lip, rhand, lhand, rpose, lpose], axis=1)\n",
    "    x = resize_pad(x)\n",
    "#     s = tf.shape(x)\n",
    "#     x = tf.reshape(x, (s[0], s[1]*s[2]))\n",
    "    x = tf.unstack(x, axis=-1)\n",
    "    x = tf.concat(x, axis=-1)\n",
    "    \n",
    "    x = tf.where(tf.math.is_nan(x), 0.0, x)\n",
    "    return x\n",
    "\n",
    "pre0 = pre_process0(frames)\n",
    "pre1 = pre_process1(*pre0)\n",
    "INPUT_SHAPE = list(pre1.shape)\n",
    "print(INPUT_SHAPE)\n",
    "#pre1\n",
    "#INPUT_SHAPE = [128, 276]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c01ae705",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T06:05:50.223596Z",
     "iopub.status.busy": "2023-08-25T06:05:50.221935Z",
     "iopub.status.idle": "2023-08-25T06:05:50.734972Z",
     "shell.execute_reply": "2023-08-25T06:05:50.734048Z"
    },
    "papermill": {
     "duration": 0.526963,
     "end_time": "2023-08-25T06:05:50.737275",
     "exception": false,
     "start_time": "2023-08-25T06:05:50.210312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_len: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nbatch = next(iter(val_dataset))\\nbatch[0].shape, batch[1].shape\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode_fn(record_bytes):\n",
    "    schema = {\n",
    "        \"lip\": tf.io.VarLenFeature(tf.float32),\n",
    "        \"rhand\": tf.io.VarLenFeature(tf.float32),\n",
    "        \"lhand\": tf.io.VarLenFeature(tf.float32),\n",
    "        \"rpose\": tf.io.VarLenFeature(tf.float32),\n",
    "        \"lpose\": tf.io.VarLenFeature(tf.float32),\n",
    "        \"phrase\": tf.io.VarLenFeature(tf.int64)\n",
    "    }\n",
    "    x = tf.io.parse_single_example(record_bytes, schema)\n",
    "\n",
    "    lip = tf.reshape(tf.sparse.to_dense(x[\"lip\"]), (-1, 40, 3))\n",
    "    rhand = tf.reshape(tf.sparse.to_dense(x[\"rhand\"]), (-1, 21, 3))\n",
    "    lhand = tf.reshape(tf.sparse.to_dense(x[\"lhand\"]), (-1, 21, 3))\n",
    "    rpose = tf.reshape(tf.sparse.to_dense(x[\"rpose\"]), (-1, 5, 3))\n",
    "    lpose = tf.reshape(tf.sparse.to_dense(x[\"lpose\"]), (-1, 5, 3))\n",
    "    phrase = tf.sparse.to_dense(x[\"phrase\"])\n",
    "\n",
    "    return lip, rhand, lhand, rpose, lpose, phrase\n",
    "\n",
    "def pre_process_fn(lip, rhand, lhand, rpose, lpose, phrase):\n",
    "    phrase = tf.pad(phrase, [[0, MAX_PHRASE_LENGTH-tf.shape(phrase)[0]]], constant_values=pad_token_idx)\n",
    "    return pre_process1(lip, rhand, lhand, rpose, lpose), phrase\n",
    "    \n",
    "tffiles = [f\"/kaggle/input/personnal-data-3/tfds/{file_id}.tfrecord\" for file_id in df.file_id.unique()]\n",
    "val_len = 1#int(0.05 * len(tffiles))\n",
    "print('val_len: ' + str(val_len))\n",
    "# train_batch_size = 256\n",
    "# val_batch_size = 256\n",
    "'''\n",
    "train_dataset =  tf.data.TFRecordDataset(tffiles).prefetch(tf.data.AUTOTUNE).shuffle(5000).map(decode_fn, num_parallel_calls=tf.data.AUTOTUNE).map(pre_process_fn, num_parallel_calls=tf.data.AUTOTUNE).batch(train_batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset =  tf.data.TFRecordDataset(tffiles[:val_len]).prefetch(tf.data.AUTOTUNE).map(decode_fn, num_parallel_calls=tf.data.AUTOTUNE).map(pre_process_fn, num_parallel_calls=tf.data.AUTOTUNE).batch(train_batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "'''\n",
    "train_dataset_pre =  tf.data.TFRecordDataset(tffiles[val_len:]).prefetch(tf.data.AUTOTUNE).map(decode_fn, num_parallel_calls=tf.data.AUTOTUNE).map(pre_process_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_dataset_pre =  tf.data.TFRecordDataset(tffiles[:val_len]).prefetch(tf.data.AUTOTUNE).map(decode_fn, num_parallel_calls=tf.data.AUTOTUNE).map(pre_process_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "'''\n",
    "batch = next(iter(val_dataset))\n",
    "batch[0].shape, batch[1].shape\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "710e54e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T06:05:50.760892Z",
     "iopub.status.busy": "2023-08-25T06:05:50.760535Z",
     "iopub.status.idle": "2023-08-25T06:05:50.765425Z",
     "shell.execute_reply": "2023-08-25T06:05:50.764467Z"
    },
    "papermill": {
     "duration": 0.019128,
     "end_time": "2023-08-25T06:05:50.767542",
     "exception": false,
     "start_time": "2023-08-25T06:05:50.748414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# val_items = [x for x in val_dataset_pre]\n",
    "# val_items_X = [x[0] for x in val_items]\n",
    "# val_items_y = [tf.cast(x[1], dtype = tf.int32) for x in val_items]\n",
    "# val_dataset = tf.data.Dataset.from_tensor_slices((val_items_X, val_items_y)).prefetch(tf.data.AUTOTUNE).batch(val_batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# train_items = [x for x in train_dataset_pre]\n",
    "# train_items_X = [x[0] for x in train_items]\n",
    "# train_items_y = [tf.cast(x[1], dtype = tf.int32) for x in train_items]\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((train_items_X, train_items_y)).prefetch(tf.data.AUTOTUNE).shuffle(60000).batch(train_batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# batch = next(iter(val_dataset))\n",
    "# batch[0].shape, batch[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fbf6e68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T06:05:50.790001Z",
     "iopub.status.busy": "2023-08-25T06:05:50.789714Z",
     "iopub.status.idle": "2023-08-25T06:05:50.793708Z",
     "shell.execute_reply": "2023-08-25T06:05:50.792659Z"
    },
    "papermill": {
     "duration": 0.017632,
     "end_time": "2023-08-25T06:05:50.795886",
     "exception": false,
     "start_time": "2023-08-25T06:05:50.778254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b6d9a21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T06:05:50.818409Z",
     "iopub.status.busy": "2023-08-25T06:05:50.818107Z",
     "iopub.status.idle": "2023-08-25T06:05:50.823329Z",
     "shell.execute_reply": "2023-08-25T06:05:50.822443Z"
    },
    "papermill": {
     "duration": 0.018408,
     "end_time": "2023-08-25T06:05:50.825278",
     "exception": false,
     "start_time": "2023-08-25T06:05:50.806870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "940875a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T06:05:50.848098Z",
     "iopub.status.busy": "2023-08-25T06:05:50.847825Z",
     "iopub.status.idle": "2023-08-25T06:05:50.857643Z",
     "shell.execute_reply": "2023-08-25T06:05:50.856767Z"
    },
    "papermill": {
     "duration": 0.023796,
     "end_time": "2023-08-25T06:05:50.859750",
     "exception": false,
     "start_time": "2023-08-25T06:05:50.835954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #Copied from previous comp 1st place model: https://www.kaggle.com/code/hoyso48/1st-place-solution-training\n",
    "# class ECA(tf.keras.layers.Layer):\n",
    "#     def __init__(self, kernel_size=5,dropout_rate=0.2, **kwargs):\n",
    "#         super().__init__(**kwargs)\n",
    "#         self.supports_masking = True\n",
    "#         self.kernel_size = kernel_size\n",
    "#         self.conv = tf.keras.layers.Conv1D(1, kernel_size=kernel_size, strides=1, padding=\"same\", use_bias=False)\n",
    "#         self.dropout = tf.keras.layers.Dropout(dropout_rate)  # Ajout de la couche de dropout\n",
    "#     def call(self, inputs, mask=None, training=None):  #training pour choisir quand faut activer drop_out (pendant training)\n",
    "#         nn = tf.keras.layers.GlobalAveragePooling1D()(inputs, mask=mask)\n",
    "#         nn = tf.expand_dims(nn, -1)\n",
    "#         nn = self.conv(nn)\n",
    "#         nn = tf.squeeze(nn, -1)\n",
    "#         nn = tf.nn.sigmoid(nn)\n",
    "#         nn = nn[:,None,:]\n",
    "#         nn = self.dropout(nn, training=training) \n",
    "\n",
    "#         return inputs * nn\n",
    "\n",
    "# class CausalDWConv1D(tf.keras.layers.Layer):\n",
    "#     def __init__(self, \n",
    "#         kernel_size=17,\n",
    "#         dilation_rate=1,\n",
    "#         use_bias=False,\n",
    "#         depthwise_initializer='glorot_uniform',\n",
    "#         dropout_rate=0.2,  # Ajout du taux de dropout\n",
    "#         name='', **kwargs):\n",
    "#         super().__init__(name=name,**kwargs)\n",
    "#         self.causal_pad = tf.keras.layers.ZeroPadding1D((dilation_rate*(kernel_size-1),0),name=name + '_pad')\n",
    "#         self.dw_conv = tf.keras.layers.DepthwiseConv1D(\n",
    "#                             kernel_size,\n",
    "#                             strides=1,\n",
    "#                             dilation_rate=dilation_rate,\n",
    "#                             padding='valid',\n",
    "#                             use_bias=use_bias,\n",
    "#                             depthwise_initializer=depthwise_initializer,\n",
    "#                             name=name + '_dwconv')\n",
    "#         self.dropout = tf.keras.layers.Dropout(dropout_rate)  # Ajout de la couche de dropout\n",
    "#         self.supports_masking = True\n",
    "        \n",
    "#     def call(self, inputs, training=None):\n",
    "#         x = self.causal_pad(inputs)\n",
    "#         x = self.dw_conv(x)\n",
    "#         x = self.dropout(x, training=training)  \n",
    "#         return x\n",
    "\n",
    "# def Conv1DBlock(channel_size,\n",
    "#           kernel_size,\n",
    "#           dilation_rate=1,\n",
    "#           drop_rate=0.0,\n",
    "#           expand_ratio=2,\n",
    "#           se_ratio=0.25,\n",
    "#           activation='swish',\n",
    "#           name=None):\n",
    "#     '''\n",
    "#     efficient conv1d block, @hoyso48\n",
    "#     '''\n",
    "#     if name is None:\n",
    "#         name = str(tf.keras.backend.get_uid(\"mbblock\"))\n",
    "#     # Expansion phase\n",
    "#     def apply(inputs):\n",
    "#         channels_in = tf.keras.backend.int_shape(inputs)[-1]\n",
    "#         channels_expand = channels_in * expand_ratio\n",
    "\n",
    "#         skip = inputs\n",
    "\n",
    "#         x = tf.keras.layers.Dense(\n",
    "#             channels_expand,\n",
    "#             use_bias=True,\n",
    "#             activation=activation,\n",
    "#             name=name + '_expand_conv')(inputs)\n",
    "\n",
    "#         # Depthwise Convolution\n",
    "#         x = CausalDWConv1D(kernel_size,\n",
    "#             dilation_rate=dilation_rate,\n",
    "#             use_bias=False,\n",
    "#             name=name + '_dwconv')(x)\n",
    "\n",
    "#         x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + '_bn')(x)\n",
    "\n",
    "#         x  = ECA()(x)\n",
    "\n",
    "#         x = tf.keras.layers.Dense(\n",
    "#             channel_size,\n",
    "#             use_bias=True,\n",
    "#             name=name + '_project_conv')(x)\n",
    "\n",
    "#         if drop_rate > 0:\n",
    "#             x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1), name=name + '_drop')(x)\n",
    "\n",
    "#         if (channels_in == channel_size):\n",
    "#             x = tf.keras.layers.add([x, skip], name=name + '_add')\n",
    "#         return x\n",
    "\n",
    "#     return apply\n",
    "\n",
    "# class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "#     def __init__(self, dim=384, num_heads=4, dropout=0, **kwargs):\n",
    "#         super().__init__(**kwargs)\n",
    "#         self.dim = dim\n",
    "#         self.scale = self.dim ** -0.5\n",
    "#         self.num_heads = num_heads\n",
    "#         self.qkv = tf.keras.layers.Dense(3 * dim, use_bias=False)\n",
    "#         self.drop1 = tf.keras.layers.Dropout(dropout)\n",
    "#         self.proj = tf.keras.layers.Dense(dim, use_bias=False)\n",
    "#         self.supports_masking = True\n",
    "\n",
    "#     def call(self, inputs, mask=None):\n",
    "#         qkv = self.qkv(inputs)\n",
    "#         qkv = tf.keras.layers.Permute((2, 1, 3))(tf.keras.layers.Reshape((-1, self.num_heads, self.dim * 3 // self.num_heads))(qkv))\n",
    "#         q, k, v = tf.split(qkv, [self.dim // self.num_heads] * 3, axis=-1)\n",
    "\n",
    "#         attn = tf.matmul(q, k, transpose_b=True) * self.scale\n",
    "\n",
    "#         if mask is not None:\n",
    "#             mask = mask[:, None, None, :]\n",
    "\n",
    "#         attn = tf.keras.layers.Softmax(axis=-1)(attn, mask=mask)\n",
    "#         attn = self.drop1(attn)\n",
    "\n",
    "#         x = attn @ v\n",
    "#         x = tf.keras.layers.Reshape((-1, self.dim))(tf.keras.layers.Permute((2, 1, 3))(x))\n",
    "#         x = self.proj(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# def TransformerBlock(dim=384, num_heads=8, expand=4, attn_dropout=0.2, drop_rate=0.2, activation='swish'):\n",
    "#     def apply(inputs):\n",
    "#         x = inputs\n",
    "#         x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "#         x = MultiHeadSelfAttention(dim=dim,num_heads=num_heads,dropout=attn_dropout)(x)\n",
    "#         x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1))(x)\n",
    "#         x = tf.keras.layers.Add()([inputs, x])\n",
    "#         attn_out = x\n",
    "\n",
    "#         x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "#         x = tf.keras.layers.Dense(dim*expand, use_bias=False, activation=activation)(x)\n",
    "#         x = tf.keras.layers.Dense(dim, use_bias=False)(x)\n",
    "#         x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1))(x)\n",
    "#         x = tf.keras.layers.Add()([attn_out, x])\n",
    "#         return x\n",
    "#     return apply\n",
    "\n",
    "# def positional_encoding(MAX_PHRASE_LENGTH, num_hid):\n",
    "#         depth = num_hid/2\n",
    "#         positions = tf.range(MAX_PHRASE_LENGTH, dtype = tf.float32)[..., tf.newaxis]\n",
    "#         depths = tf.range(depth, dtype = tf.float32)[np.newaxis, :]/depth\n",
    "#         angle_rates = tf.math.divide(1, tf.math.pow(tf.cast(10000, tf.float32), depths))\n",
    "#         angle_rads = tf.linalg.matmul(positions, angle_rates)\n",
    "#         pos_encoding = tf.concat(\n",
    "#           [tf.math.sin(angle_rads), tf.math.cos(angle_rads)],\n",
    "#           axis=-1)\n",
    "#         return pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b5537c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T06:05:50.883549Z",
     "iopub.status.busy": "2023-08-25T06:05:50.883298Z",
     "iopub.status.idle": "2023-08-25T06:05:50.914040Z",
     "shell.execute_reply": "2023-08-25T06:05:50.913212Z"
    },
    "papermill": {
     "duration": 0.045405,
     "end_time": "2023-08-25T06:05:50.916145",
     "exception": false,
     "start_time": "2023-08-25T06:05:50.870740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import typing\n",
    "\n",
    "# def shape_list(x, out_type=tf.int32):\n",
    "#     \"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\n",
    "#     static = x.shape.as_list()\n",
    "#     dynamic = tf.shape(x, out_type=out_type)\n",
    "#     return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n",
    "\n",
    "\n",
    "# def get_shape_invariants(tensor):\n",
    "#     shapes = shape_list(tensor)\n",
    "#     return tf.TensorShape([i if isinstance(i, int) else None for i in shapes])\n",
    "\n",
    "\n",
    "# def get_float_spec(tensor):\n",
    "#     shape = get_shape_invariants(tensor)\n",
    "#     return tf.TensorSpec(shape, dtype=tf.float32)\n",
    "\n",
    "# class GLU(tf.keras.layers.Layer):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         axis=-1,\n",
    "#         name=\"glu_activation\",\n",
    "#         **kwargs,\n",
    "#     ):\n",
    "#         super(GLU, self).__init__(name=name, **kwargs)\n",
    "#         self.axis = axis\n",
    "\n",
    "#     def call(\n",
    "#         self,\n",
    "#         inputs,\n",
    "#         **kwargs,\n",
    "#     ):\n",
    "#         a, b = tf.split(inputs, 2, axis=self.axis)\n",
    "#         b = tf.nn.sigmoid(b)\n",
    "#         return tf.multiply(a, b)\n",
    "\n",
    "#     def get_config(self):\n",
    "#         conf = super(GLU, self).get_config()\n",
    "#         conf.update({\"axis\": self.axis})\n",
    "#         return conf\n",
    "\n",
    "    \n",
    "# class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         num_heads,\n",
    "#         head_size,\n",
    "#         output_size: int = None,\n",
    "#         dropout: float = 0.0,\n",
    "#         use_projection_bias: bool = True,\n",
    "#         return_attn_coef: bool = False,\n",
    "#         kernel_initializer: typing.Union[str, typing.Callable] = tf.keras.initializers.glorot_uniform(seed=SEED),\n",
    "#         kernel_regularizer: typing.Union[str, typing.Callable] = None,\n",
    "#         kernel_constraint: typing.Union[str, typing.Callable] = None,\n",
    "#         bias_initializer: typing.Union[str, typing.Callable] = \"zeros\",\n",
    "#         bias_regularizer: typing.Union[str, typing.Callable] = None,\n",
    "#         bias_constraint: typing.Union[str, typing.Callable] = None,\n",
    "#         **kwargs,\n",
    "#     ):\n",
    "#         super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "\n",
    "#         if output_size is not None and output_size < 1:\n",
    "#             raise ValueError(\"output_size must be a positive number\")\n",
    "\n",
    "#         self.kernel_initializer = tf.keras.initializers.get(kernel_initializer)\n",
    "#         self.kernel_regularizer = tf.keras.regularizers.get(kernel_regularizer)\n",
    "#         self.kernel_constraint = tf.keras.constraints.get(kernel_constraint)\n",
    "#         self.bias_initializer = tf.keras.initializers.get(bias_initializer)\n",
    "#         self.bias_regularizer = tf.keras.regularizers.get(bias_regularizer)\n",
    "#         self.bias_constraint = tf.keras.constraints.get(bias_constraint)\n",
    "\n",
    "#         self.head_size = head_size\n",
    "#         self.num_heads = num_heads\n",
    "#         self.output_size = output_size\n",
    "#         self.use_projection_bias = use_projection_bias\n",
    "#         self.return_attn_coef = return_attn_coef\n",
    "\n",
    "#         self.dropout = tf.keras.layers.Dropout(dropout, name=\"dropout\")\n",
    "#         self._droput_rate = dropout\n",
    "#         self.supports_masking = True # RAJOUT\n",
    "\n",
    "#     def build(\n",
    "#         self,\n",
    "#         input_shape,\n",
    "#     ):\n",
    "#         num_query_features = input_shape[0][-1]\n",
    "#         num_key_features = input_shape[1][-1]\n",
    "#         num_value_features = input_shape[2][-1] if len(input_shape) > 2 else num_key_features\n",
    "#         output_size = self.output_size if self.output_size is not None else num_value_features\n",
    "#         self.query_kernel = self.add_weight(\n",
    "#             name=\"query_kernel\",\n",
    "#             shape=[self.num_heads, num_query_features, self.head_size],\n",
    "#             initializer=self.kernel_initializer,\n",
    "#             regularizer=self.kernel_regularizer,\n",
    "#             constraint=self.kernel_constraint,\n",
    "#         )\n",
    "#         self.key_kernel = self.add_weight(\n",
    "#             name=\"key_kernel\",\n",
    "#             shape=[self.num_heads, num_key_features, self.head_size],\n",
    "#             initializer=self.kernel_initializer,\n",
    "#             regularizer=self.kernel_regularizer,\n",
    "#             constraint=self.kernel_constraint,\n",
    "#         )\n",
    "#         self.value_kernel = self.add_weight(\n",
    "#             name=\"value_kernel\",\n",
    "#             shape=[self.num_heads, num_value_features, self.head_size],\n",
    "#             initializer=self.kernel_initializer,\n",
    "#             regularizer=self.kernel_regularizer,\n",
    "#             constraint=self.kernel_constraint,\n",
    "#         )\n",
    "#         self.projection_kernel = self.add_weight(\n",
    "#             name=\"projection_kernel\",\n",
    "#             shape=[self.num_heads, self.head_size, output_size],\n",
    "#             initializer=self.kernel_initializer,\n",
    "#             regularizer=self.kernel_regularizer,\n",
    "#             constraint=self.kernel_constraint,\n",
    "#         )\n",
    "#         if self.use_projection_bias:\n",
    "#             self.projection_bias = self.add_weight(\n",
    "#                 name=\"projection_bias\",\n",
    "#                 shape=[output_size],\n",
    "#                 initializer=self.bias_initializer,\n",
    "#                 regularizer=self.bias_regularizer,\n",
    "#                 constraint=self.bias_constraint,\n",
    "#             )\n",
    "#         else:\n",
    "#             self.projection_bias = None\n",
    "\n",
    "#     def call_qkv(\n",
    "#         self,\n",
    "#         query,\n",
    "#         key,\n",
    "#         value,\n",
    "#         training=None,\n",
    "#     ):\n",
    "#         # verify shapes\n",
    "#         if key.shape[-2] != value.shape[-2]:\n",
    "#             raise ValueError(\n",
    "#                 \"the number of elements in 'key' must be equal to \" \"the same as the number of elements in 'value'\"\n",
    "#             )\n",
    "#         # Linear transformations\n",
    "#         query = tf.einsum(\"...NI,HIO->...NHO\", query, self.query_kernel)\n",
    "#         key = tf.einsum(\"...MI,HIO->...MHO\", key, self.key_kernel)\n",
    "#         value = tf.einsum(\"...MI,HIO->...MHO\", value, self.value_kernel)\n",
    "\n",
    "#         return query, key, value\n",
    "\n",
    "#     def call_attention(\n",
    "#         self,\n",
    "#         query,\n",
    "#         key,\n",
    "#         value,\n",
    "#         logits,\n",
    "#         training=None,\n",
    "#         mask=None,\n",
    "#     ):\n",
    "#         # mask = attention mask with shape [B, Tquery, Tkey] with 1 is for positions we want to attend, 0 for masked\n",
    "#         if mask is not None:\n",
    "#             if len(mask.shape) < 2: #was written mask \n",
    "#                 raise ValueError(\"'mask' must have at least 2 dimensions\")\n",
    "#             if query.shape[-3] != mask.shape[-2]:\n",
    "#                 raise ValueError(\"mask's second to last dimension must be equal to \" \"the number of elements in 'query'\")\n",
    "#             if key.shape[-3] != mask.shape[-1]:\n",
    "#                 raise ValueError(\"mask's last dimension must be equal to the number of elements in 'key'\")\n",
    "#         # apply mask\n",
    "#         if mask is not None:\n",
    "#             mask = tf.cast(mask, tf.float32)\n",
    "\n",
    "#             # possibly expand on the head dimension so broadcasting works\n",
    "#             if len(mask.shape) != len(logits.shape):\n",
    "#                 mask = tf.expand_dims(mask, -3)\n",
    "\n",
    "#             logits += -10e9 * (1.0 - mask)\n",
    "\n",
    "#         attn_coef = tf.nn.softmax(logits)\n",
    "\n",
    "#         # attention dropout\n",
    "#         attn_coef_dropout = self.dropout(attn_coef, training=training)\n",
    "\n",
    "#         # attention * value\n",
    "#         multihead_output = tf.einsum(\"...HNM,...MHI->...NHI\", attn_coef_dropout, value)\n",
    "\n",
    "#         # Run the outputs through another linear projection layer. Recombining heads\n",
    "#         # is automatically done.\n",
    "#         output = tf.einsum(\"...NHI,HIO->...NO\", multihead_output, self.projection_kernel)\n",
    "\n",
    "#         if self.projection_bias is not None:\n",
    "#             output += self.projection_bias\n",
    "\n",
    "#         return output, attn_coef\n",
    "\n",
    "#     def call(\n",
    "#         self,\n",
    "#         inputs,\n",
    "#         training=None,\n",
    "#         mask=None,\n",
    "#         **kwargs,\n",
    "#     ):\n",
    "#         query, key, value = inputs\n",
    "\n",
    "#         query, key, value = self.call_qkv(query, key, value, training=training)\n",
    "\n",
    "#         # Scale dot-product, doing the division to either query or key\n",
    "#         # instead of their product saves some computation\n",
    "#         depth = tf.constant(self.head_size, dtype=tf.float32)\n",
    "#         query /= tf.sqrt(depth)\n",
    "\n",
    "#         # Calculate dot product attention\n",
    "#         logits = tf.einsum(\"...NHO,...MHO->...HNM\", query, key)\n",
    "\n",
    "#         output, attn_coef = self.call_attention(query, key, value, logits, training=training, mask=mask)\n",
    "\n",
    "#         if self.return_attn_coef:\n",
    "#             return output, attn_coef\n",
    "#         else:\n",
    "#             return output\n",
    "\n",
    "#     def compute_output_shape(\n",
    "#         self,\n",
    "#         input_shape,\n",
    "#     ):\n",
    "#         num_value_features = input_shape[2][-1] if len(input_shape) > 2 else input_shape[1][-1]\n",
    "#         output_size = self.output_size if self.output_size is not None else num_value_features\n",
    "\n",
    "#         output_shape = input_shape[0][:-1] + (output_size,)\n",
    "\n",
    "#         if self.return_attn_coef:\n",
    "#             num_query_elements = input_shape[0][-2]\n",
    "#             num_key_elements = input_shape[1][-2]\n",
    "#             attn_coef_shape = input_shape[0][:-2] + (\n",
    "#                 self.num_heads,\n",
    "#                 num_query_elements,\n",
    "#                 num_key_elements,\n",
    "#             )\n",
    "\n",
    "#             return output_shape, attn_coef_shape\n",
    "#         else:\n",
    "#             return output_shape\n",
    "\n",
    "#     def get_config(self):\n",
    "#         config = super().get_config()\n",
    "\n",
    "#         config.update(\n",
    "#             head_size=self.head_size,\n",
    "#             num_heads=self.num_heads,\n",
    "#             output_size=self.output_size,\n",
    "#             dropout=self._droput_rate,\n",
    "#             use_projection_bias=self.use_projection_bias,\n",
    "#             return_attn_coef=self.return_attn_coef,\n",
    "#             kernel_initializer=tf.keras.initializers.serialize(self.kernel_initializer),\n",
    "#             kernel_regularizer=tf.keras.regularizers.serialize(self.kernel_regularizer),\n",
    "#             kernel_constraint=tf.keras.constraints.serialize(self.kernel_constraint),\n",
    "#             bias_initializer=tf.keras.initializers.serialize(self.bias_initializer),\n",
    "#             bias_regularizer=tf.keras.regularizers.serialize(self.bias_regularizer),\n",
    "#             bias_constraint=tf.keras.constraints.serialize(self.bias_constraint),\n",
    "#         )\n",
    "\n",
    "#         return config\n",
    "\n",
    "\n",
    "# class RelPositionMultiHeadAttention(MultiHeadAttention):\n",
    "#     def build(\n",
    "#         self,\n",
    "#         input_shape,\n",
    "#     ):\n",
    "#         num_pos_features = input_shape[-1][-1]\n",
    "#         self.pos_kernel = self.add_weight(\n",
    "#             name=\"pos_kernel\",\n",
    "#             shape=[self.num_heads, num_pos_features, self.head_size],\n",
    "#             initializer=self.kernel_initializer,\n",
    "#             regularizer=self.kernel_regularizer,\n",
    "#             constraint=self.kernel_constraint,\n",
    "#         )\n",
    "#         self.pos_bias_u = self.add_weight(\n",
    "#             name=\"pos_bias_u\",\n",
    "#             shape=[self.num_heads, self.head_size],\n",
    "#             regularizer=self.kernel_regularizer,\n",
    "#             initializer=self.kernel_initializer,\n",
    "#             constraint=self.kernel_constraint,\n",
    "#         )\n",
    "#         self.pos_bias_v = self.add_weight(\n",
    "#             name=\"pos_bias_v\",\n",
    "#             shape=[self.num_heads, self.head_size],\n",
    "#             regularizer=self.kernel_regularizer,\n",
    "#             initializer=self.kernel_initializer,\n",
    "#             constraint=self.kernel_constraint,\n",
    "#         )\n",
    "#         super(RelPositionMultiHeadAttention, self).build(input_shape[:-1])\n",
    "\n",
    "#     @staticmethod\n",
    "#     def relative_shift(x):\n",
    "#         x_shape = tf.shape(x)\n",
    "#         x = tf.pad(x, [[0, 0], [0, 0], [0, 0], [1, 0]])\n",
    "#         x = tf.reshape(x, [x_shape[0], x_shape[1], x_shape[3] + 1, x_shape[2]])\n",
    "#         x = tf.reshape(x[:, :, 1:, :], x_shape)\n",
    "#         return x\n",
    "\n",
    "#     def call(\n",
    "#         self,\n",
    "#         inputs,\n",
    "#         training=None,\n",
    "#         mask=None,\n",
    "#         **kwargs,\n",
    "#     ):\n",
    "#         query, key, value, pos = inputs\n",
    "\n",
    "#         query, key, value = self.call_qkv(query, key, value, training=training)\n",
    "\n",
    "#         pos = tf.einsum(\"...MI,HIO->...MHO\", pos, self.pos_kernel)\n",
    "\n",
    "#         query_with_u = query + self.pos_bias_u\n",
    "#         query_with_v = query + self.pos_bias_v\n",
    "\n",
    "#         logits_with_u = tf.einsum(\"...NHO,...MHO->...HNM\", query_with_u, key)\n",
    "#         logits_with_v = tf.einsum(\"...NHO,...MHO->...HNM\", query_with_v, pos)\n",
    "#         logits_with_v = self.relative_shift(logits_with_v)\n",
    "\n",
    "#         logits = logits_with_u + logits_with_v[:, :, :, : tf.shape(logits_with_u)[3]]\n",
    "\n",
    "#         depth = tf.constant(self.head_size, dtype=tf.float32)\n",
    "#         logits /= tf.sqrt(depth)\n",
    "\n",
    "#         output, attn_coef = self.call_attention(query, key, value, logits, training=training, mask=mask)\n",
    "\n",
    "#         if self.return_attn_coef:\n",
    "#             return output, attn_coef\n",
    "#         else:\n",
    "#             return output\n",
    "        \n",
    "        \n",
    "# class PositionalEncoding(tf.keras.layers.Layer):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         alpha: int = 1,\n",
    "#         beta: int = 0,\n",
    "#         name=\"positional_encoding\",\n",
    "#         **kwargs,\n",
    "#     ):\n",
    "#         super().__init__(trainable=False, name=name, **kwargs)\n",
    "#         self.alpha = alpha\n",
    "#         self.beta = beta\n",
    "#         self.supports_masking = True # RAJOUT\n",
    "\n",
    "#     def build(\n",
    "#         self,\n",
    "#         input_shape,\n",
    "#     ):\n",
    "#         dmodel = input_shape[-1]\n",
    "#         assert dmodel % 2 == 0, f\"Input last dim must be even: {dmodel}\"\n",
    "\n",
    "#     @staticmethod\n",
    "#     def encode(\n",
    "#         max_len,\n",
    "#         dmodel,\n",
    "#     ):\n",
    "#         pos = tf.expand_dims(tf.range(max_len - 1, -1, -1.0, dtype=tf.float32), axis=1)\n",
    "#         index = tf.expand_dims(tf.range(0, dmodel, dtype=tf.float32), axis=0)\n",
    "\n",
    "#         pe = pos * (1 / tf.pow(10000.0, (2 * (index // 2)) / dmodel))\n",
    "\n",
    "#         # Sin cos will be [max_len, size // 2]\n",
    "#         # we add 0 between numbers by using padding and reshape\n",
    "#         sin = tf.pad(tf.expand_dims(tf.sin(pe[:, 0::2]), -1), [[0, 0], [0, 0], [0, 1]], mode=\"CONSTANT\", constant_values=0)\n",
    "#         sin = tf.reshape(sin, [max_len, dmodel])\n",
    "#         cos = tf.pad(tf.expand_dims(tf.cos(pe[:, 1::2]), -1), [[0, 0], [0, 0], [1, 0]], mode=\"CONSTANT\", constant_values=0)\n",
    "#         cos = tf.reshape(cos, [max_len, dmodel])\n",
    "#         # Then add sin and cos, which results in [time, size]\n",
    "#         pe = tf.add(sin, cos)\n",
    "#         return tf.expand_dims(pe, axis=0)  # [1, time, size]\n",
    "\n",
    "#     def call(\n",
    "#         self,\n",
    "#         inputs,\n",
    "#         **kwargs,\n",
    "#     ):\n",
    "#         # inputs shape [B, T, V]\n",
    "#         _, max_len, dmodel = shape_list(inputs)\n",
    "#         pe = self.encode(max_len * self.alpha + self.beta, dmodel)\n",
    "#         return tf.cast(pe, dtype=inputs.dtype)\n",
    "\n",
    "#     def get_config(self):\n",
    "#         conf = super().get_config()\n",
    "#         conf.update({\"alpha\": self.alpha, \"beta\": self.beta})\n",
    "#         return conf\n",
    "\n",
    "\n",
    "# class PositionalEncodingConcat(PositionalEncoding):\n",
    "#     def build(\n",
    "#         self,\n",
    "#         input_shape,\n",
    "#     ):\n",
    "#         dmodel = input_shape[-1]\n",
    "#         assert dmodel % 2 == 0, f\"Input last dim must be even: {dmodel}\"\n",
    "\n",
    "#     @staticmethod\n",
    "#     def encode(\n",
    "#         max_len,\n",
    "#         dmodel,\n",
    "#     ):\n",
    "#         pos = tf.range(max_len - 1, -1, -1.0, dtype=tf.float32)\n",
    "\n",
    "#         index = tf.range(0, dmodel, 2.0, dtype=tf.float32)\n",
    "#         index = 1 / tf.pow(10000.0, (index / dmodel))\n",
    "\n",
    "#         sinusoid = tf.einsum(\"i,j->ij\", pos, index)\n",
    "#         pos = tf.concat([tf.sin(sinusoid), tf.cos(sinusoid)], axis=-1)\n",
    "\n",
    "#         return tf.expand_dims(pos, axis=0)\n",
    "\n",
    "#     def call(\n",
    "#         self,\n",
    "#         inputs,\n",
    "#         **kwargs,\n",
    "#     ):\n",
    "#         # inputs shape [B, T, V]\n",
    "#         _, max_len, dmodel = shape_list(inputs)\n",
    "#         pe = self.encode(max_len * self.alpha + self.beta, dmodel)\n",
    "#         return tf.cast(pe, dtype=inputs.dtype)\n",
    "    \n",
    "    \n",
    "\n",
    "# L2 = tf.keras.regularizers.l2(1e-6)\n",
    "\n",
    "\n",
    "# class FFModule(tf.keras.layers.Layer):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         input_dim,\n",
    "#         dropout=0.0,\n",
    "#         fc_factor=0.5,\n",
    "#         kernel_regularizer=L2,\n",
    "#         bias_regularizer=L2,\n",
    "#         name=\"ff_module\",\n",
    "#         **kwargs,\n",
    "#     ):\n",
    "#         super(FFModule, self).__init__(name=name, **kwargs)\n",
    "#         self.fc_factor = fc_factor\n",
    "#         self.ln = tf.keras.layers.LayerNormalization(\n",
    "#             name=f\"{name}_ln\",\n",
    "#             gamma_regularizer=kernel_regularizer,\n",
    "#             beta_regularizer=bias_regularizer,\n",
    "#         )\n",
    "#         self.ffn1 = tf.keras.layers.Dense(\n",
    "#             4 * input_dim,\n",
    "#             name=f\"{name}_dense_1\",\n",
    "#             kernel_regularizer=kernel_regularizer,\n",
    "#             bias_regularizer=bias_regularizer,\n",
    "#         )\n",
    "#         self.swish = tf.keras.layers.Activation(tf.nn.swish, name=f\"{name}_swish_activation\")\n",
    "#         self.do1 = tf.keras.layers.Dropout(dropout, name=f\"{name}_dropout_1\")\n",
    "#         self.ffn2 = tf.keras.layers.Dense(\n",
    "#             input_dim,\n",
    "#             name=f\"{name}_dense_2\",\n",
    "#             kernel_regularizer=kernel_regularizer,\n",
    "#             bias_regularizer=bias_regularizer,\n",
    "#         )\n",
    "#         self.do2 = tf.keras.layers.Dropout(dropout, name=f\"{name}_dropout_2\")\n",
    "#         self.res_add = tfa.layers.StochasticDepth(name=f\"{name}_add\", survival_probability=0.4)\n",
    "#         self.supports_masking = True # RAJOUT\n",
    "\n",
    "#     def call(\n",
    "#         self,\n",
    "#         inputs,\n",
    "#         training=None,\n",
    "#         **kwargs,\n",
    "#     ):\n",
    "#         outputs = self.ln(inputs, training=training)\n",
    "#         outputs = self.ffn1(outputs, training=training)\n",
    "#         outputs = self.swish(outputs)\n",
    "#         outputs = self.do1(outputs, training=training)\n",
    "#         outputs = self.ffn2(outputs, training=training)\n",
    "#         outputs = self.do2(outputs, training=training)\n",
    "#         outputs = self.res_add([inputs, self.fc_factor * outputs])\n",
    "#         return outputs\n",
    "\n",
    "#     def get_config(self):\n",
    "#         conf = super(FFModule, self).get_config()\n",
    "#         conf.update({\"fc_factor\": self.fc_factor})\n",
    "#         conf.update(self.ln.get_config())\n",
    "#         conf.update(self.ffn1.get_config())\n",
    "#         conf.update(self.swish.get_config())\n",
    "#         conf.update(self.do1.get_config())\n",
    "#         conf.update(self.ffn2.get_config())\n",
    "#         conf.update(self.do2.get_config())\n",
    "#         conf.update(self.res_add.get_config())\n",
    "#         return conf\n",
    "\n",
    "\n",
    "# class MHSAModule(tf.keras.layers.Layer):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         head_size,\n",
    "#         num_heads,\n",
    "#         dropout=0.0,\n",
    "#         mha_type=\"relmha\",\n",
    "#         kernel_regularizer=L2,\n",
    "#         bias_regularizer=L2,\n",
    "#         name=\"mhsa_module\",\n",
    "#         **kwargs,\n",
    "#     ):\n",
    "#         super(MHSAModule, self).__init__(name=name, **kwargs)\n",
    "#         self.ln = tf.keras.layers.LayerNormalization(\n",
    "#             name=f\"{name}_ln\",\n",
    "#             gamma_regularizer=kernel_regularizer,\n",
    "#             beta_regularizer=bias_regularizer,\n",
    "#         )\n",
    "#         if mha_type == \"relmha\":\n",
    "#             self.mha = RelPositionMultiHeadAttention(\n",
    "#                 name=f\"{name}_mhsa\",\n",
    "#                 head_size=head_size,\n",
    "#                 num_heads=num_heads,\n",
    "#                 kernel_regularizer=kernel_regularizer,\n",
    "#                 bias_regularizer=bias_regularizer,\n",
    "#             )\n",
    "#         elif mha_type == \"mha\":\n",
    "#             self.mha = MultiHeadAttention(\n",
    "#                 name=f\"{name}_mhsa\",\n",
    "#                 head_size=head_size,\n",
    "#                 num_heads=num_heads,\n",
    "#                 kernel_regularizer=kernel_regularizer,\n",
    "#                 bias_regularizer=bias_regularizer,\n",
    "#             )\n",
    "#         else:\n",
    "#             raise ValueError(\"mha_type must be either 'mha' or 'relmha'\")\n",
    "#         self.do = tf.keras.layers.Dropout(dropout, name=f\"{name}_dropout\")\n",
    "#         self.res_add = tfa.layers.StochasticDepth(name=f\"{name}_add\", survival_probability=0.5)\n",
    "#         self.mha_type = mha_type\n",
    "#         self.supports_masking = True # RAJOUT\n",
    "\n",
    "#     def call(\n",
    "#         self,\n",
    "#         inputs,\n",
    "#         training=None,\n",
    "#         mask=None,\n",
    "#         **kwargs,\n",
    "#     ):\n",
    "#         inputs, pos = inputs  # pos is positional encoding\n",
    "#         outputs = self.ln(inputs, training=training)\n",
    "#         if self.mha_type == \"relmha\":\n",
    "#             outputs = self.mha([outputs, outputs, outputs, pos], training=training, mask=mask)\n",
    "#         else:\n",
    "#             outputs = outputs + pos\n",
    "#             outputs = self.mha([outputs, outputs, outputs], training=training, mask=mask)\n",
    "#         outputs = self.do(outputs, training=training)\n",
    "#         outputs = self.res_add([inputs, outputs])\n",
    "#         return outputs\n",
    "\n",
    "#     def get_config(self):\n",
    "#         conf = super(MHSAModule, self).get_config()\n",
    "#         conf.update({\"mha_type\": self.mha_type})\n",
    "#         conf.update(self.ln.get_config())\n",
    "#         conf.update(self.mha.get_config())\n",
    "#         conf.update(self.do.get_config())\n",
    "#         conf.update(self.res_add.get_config())\n",
    "#         return conf\n",
    "\n",
    "\n",
    "# class ConvModule(tf.keras.layers.Layer):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         input_dim,\n",
    "#         kernel_size=32,\n",
    "#         dropout=0.0,\n",
    "#         depth_multiplier=1,\n",
    "#         kernel_regularizer=L2,\n",
    "#         bias_regularizer=L2,\n",
    "#         name=\"conv_module\",\n",
    "#         **kwargs,\n",
    "#     ):\n",
    "#         super(ConvModule, self).__init__(name=name, **kwargs)\n",
    "#         self.ln = tf.keras.layers.LayerNormalization()\n",
    "#         self.pw_conv_1 = tf.keras.layers.Conv1D(\n",
    "#             filters=2 * input_dim,\n",
    "#             kernel_size=1,\n",
    "#             strides=1,\n",
    "#             padding=\"valid\",\n",
    "#             name=f\"{name}_pw_conv_1\",\n",
    "#             kernel_regularizer=kernel_regularizer,\n",
    "#             bias_regularizer=bias_regularizer,\n",
    "#         )\n",
    "#         self.glu = GLU(name=f\"{name}_glu\")\n",
    "#         self.dw_conv = CausalDWConv1D(\n",
    "#             kernel_size=kernel_size,\n",
    "#             name=f\"{name}_dw_conv\",\n",
    "#             depth_multiplier = depth_multiplier,\n",
    "#         )\n",
    "#         self.bn = tf.keras.layers.BatchNormalization(\n",
    "#             name=f\"{name}_bn\",\n",
    "#             gamma_regularizer=kernel_regularizer,\n",
    "#             beta_regularizer=bias_regularizer,\n",
    "#         )\n",
    "#         self.swish = tf.keras.layers.Activation(\n",
    "#             tf.nn.swish,\n",
    "#             name=f\"{name}_swish_activation\",\n",
    "#         )\n",
    "#         self.pw_conv_2 = tf.keras.layers.Conv1D(\n",
    "#             filters=input_dim,\n",
    "#             kernel_size=1,\n",
    "#             strides=1,\n",
    "#             padding=\"valid\",\n",
    "#             name=f\"{name}_pw_conv_2\",\n",
    "#             kernel_regularizer=kernel_regularizer,\n",
    "#             bias_regularizer=bias_regularizer,\n",
    "#         )\n",
    "#         self.do = tf.keras.layers.Dropout(dropout, name=f\"{name}_dropout\")\n",
    "#         self.res_add = tfa.layers.StochasticDepth(name=f\"{name}_add\", survival_probability=0.5)\n",
    "\n",
    "#     def call(\n",
    "#         self,\n",
    "#         inputs,\n",
    "#         training=None,\n",
    "#         **kwargs,\n",
    "#     ):\n",
    "#         outputs = self.ln(inputs, training=training)\n",
    "#         B, T, E = shape_list(outputs)\n",
    "#         outputs = tf.reshape(outputs, [B, T, E]) # [B, T, 1, E]\n",
    "#         outputs = self.pw_conv_1(outputs, training=training)\n",
    "#         outputs = self.glu(outputs)\n",
    "#         outputs = self.dw_conv(outputs, training=training)\n",
    "#         outputs = self.bn(outputs, training=training)\n",
    "#         outputs = self.swish(outputs)\n",
    "#         outputs = self.pw_conv_2(outputs, training=training)\n",
    "#         outputs = tf.reshape(outputs, [B, T, E]) #\n",
    "#         outputs = self.do(outputs, training=training)\n",
    "#         outputs = self.res_add([inputs, outputs])\n",
    "#         return outputs\n",
    "\n",
    "#     def get_config(self):\n",
    "#         conf = super(ConvModule, self).get_config()\n",
    "#         conf.update(self.ln.get_config())\n",
    "#         conf.update(self.pw_conv_1.get_config())\n",
    "#         conf.update(self.glu.get_config())\n",
    "#         conf.update(self.dw_conv.get_config())\n",
    "#         conf.update(self.bn.get_config())\n",
    "#         conf.update(self.swish.get_config())\n",
    "#         conf.update(self.pw_conv_2.get_config())\n",
    "#         conf.update(self.do.get_config())\n",
    "#         conf.update(self.res_add.get_config())\n",
    "#         return conf\n",
    "    \n",
    "    \n",
    "\n",
    "# class CausalDWConv1D(tf.keras.layers.Layer):\n",
    "#     def __init__(self,\n",
    "#         kernel_size=17,\n",
    "#         dilation_rate=1,\n",
    "#         use_bias=False,\n",
    "#         depthwise_initializer=tf.keras.initializers.glorot_uniform(seed=SEED),\n",
    "#         depth_multiplier = 1,\n",
    "#         name='', **kwargs):\n",
    "#         super().__init__(name=name,**kwargs)\n",
    "#         self.causal_pad = tf.keras.layers.ZeroPadding1D((dilation_rate*(kernel_size-1),0),name=name + '_pad')\n",
    "#         self.dw_conv = tf.keras.layers.DepthwiseConv1D(\n",
    "#                             kernel_size,\n",
    "#                             strides=1,\n",
    "#                             dilation_rate=dilation_rate,\n",
    "#                             depth_multiplier=depth_multiplier,\n",
    "#                             padding='valid',\n",
    "#                             use_bias=use_bias,\n",
    "#                             depthwise_initializer=depthwise_initializer,\n",
    "#                             name=name + '_dwconv')\n",
    "#         self.supports_masking = True\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         x = self.causal_pad(inputs)\n",
    "#         x = self.dw_conv(x)\n",
    "#         return x\n",
    "    \n",
    "# class ConformerBlock(tf.keras.layers.Layer):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         input_dim,\n",
    "#         dropout=0.0,\n",
    "#         fc_factor=0.5,\n",
    "#         head_size=36,\n",
    "#         num_heads=4,\n",
    "#         mha_type=\"relmha\",\n",
    "#         kernel_size=32,\n",
    "#         depth_multiplier=1,\n",
    "#         kernel_regularizer=L2,\n",
    "#         bias_regularizer=L2,\n",
    "#         name=\"conformer_block\",\n",
    "#         **kwargs,\n",
    "#     ):\n",
    "#         super(ConformerBlock, self).__init__(name=name, **kwargs)\n",
    "#         self.ffm1 = FFModule(\n",
    "#             input_dim=input_dim,\n",
    "#             dropout=dropout,\n",
    "#             fc_factor=fc_factor,\n",
    "#             name=f\"{name}_ff_module_1\",\n",
    "#             kernel_regularizer=kernel_regularizer,\n",
    "#             bias_regularizer=bias_regularizer,\n",
    "#         )\n",
    "#         self.mhsam = MHSAModule(\n",
    "#             mha_type=mha_type,\n",
    "#             head_size=head_size,\n",
    "#             num_heads=num_heads,\n",
    "#             dropout=dropout,\n",
    "#             name=f\"{name}_mhsa_module\",\n",
    "#             kernel_regularizer=kernel_regularizer,\n",
    "#             bias_regularizer=bias_regularizer,\n",
    "#         )\n",
    "#         self.convm = ConvModule(\n",
    "#             input_dim=input_dim,\n",
    "#             kernel_size=kernel_size,\n",
    "#             dropout=dropout,\n",
    "#             name=f\"{name}_conv_module\",\n",
    "#             depth_multiplier=depth_multiplier,\n",
    "#             kernel_regularizer=kernel_regularizer,\n",
    "#             bias_regularizer=bias_regularizer,\n",
    "#         )\n",
    "#         self.ffm2 = FFModule(\n",
    "#             input_dim=input_dim,\n",
    "#             dropout=dropout,\n",
    "#             fc_factor=fc_factor,\n",
    "#             name=f\"{name}_ff_module_2\",\n",
    "#             kernel_regularizer=kernel_regularizer,\n",
    "#             bias_regularizer=bias_regularizer,\n",
    "#         )\n",
    "#         self.ln = tf.keras.layers.LayerNormalization(\n",
    "#             name=f\"{name}_ln\",\n",
    "#             gamma_regularizer=kernel_regularizer,\n",
    "#             beta_regularizer=kernel_regularizer,\n",
    "#         )\n",
    "#         self.supports_masking = True # RAJOUT\n",
    "\n",
    "#     def call(\n",
    "#         self,\n",
    "#         inputs,\n",
    "#         training=None,\n",
    "#         mask=None,\n",
    "#         **kwargs,\n",
    "#     ):\n",
    "#         inputs, pos = inputs  # pos is positional encoding\n",
    "#         outputs = self.ffm1(inputs, training=training, **kwargs)\n",
    "#         outputs = self.mhsam([outputs, pos], training=training, mask=mask, **kwargs)\n",
    "#         outputs = self.convm(outputs, training=training, **kwargs)\n",
    "#         outputs = self.ffm2(outputs, training=training, **kwargs)\n",
    "#         outputs = self.ln(outputs, training=training)\n",
    "#         return outputs\n",
    "\n",
    "#     def get_config(self):\n",
    "#         conf = super(ConformerBlock, self).get_config()\n",
    "#         conf.update(self.ffm1.get_config())\n",
    "#         conf.update(self.mhsam.get_config())\n",
    "#         conf.update(self.convm.get_config())\n",
    "#         conf.update(self.ffm2.get_config())\n",
    "#         conf.update(self.ln.get_config())\n",
    "#         return conf\n",
    "\n",
    "# def get_attention_mask(x_inp):\n",
    "#     # Attention Mask\n",
    "#     attention_mask = tf.math.count_nonzero(x_inp, axis=[2], keepdims=True, dtype=tf.int32)\n",
    "#     attention_mask = tf.math.count_nonzero(attention_mask, axis=[2], keepdims=True)\n",
    "#     attention_mask = tf.broadcast_to(attention_mask, [attention_mask.shape[0], attention_mask.shape[1], attention_mask.shape[1]])\n",
    "#     return attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a2854c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T06:05:50.938834Z",
     "iopub.status.busy": "2023-08-25T06:05:50.938556Z",
     "iopub.status.idle": "2023-08-25T06:05:51.049293Z",
     "shell.execute_reply": "2023-08-25T06:05:51.048376Z"
    },
    "papermill": {
     "duration": 0.125032,
     "end_time": "2023-08-25T06:05:51.051606",
     "exception": false,
     "start_time": "2023-08-25T06:05:50.926574",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import typing\n",
    "seed_everything()\n",
    "def shape_list(x, out_type=tf.int32):\n",
    "    \"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\n",
    "    static = x.shape.as_list()\n",
    "    dynamic = tf.shape(x, out_type=out_type)\n",
    "    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n",
    "\n",
    "\n",
    "def get_shape_invariants(tensor):\n",
    "    shapes = shape_list(tensor)\n",
    "    return tf.TensorShape([i if isinstance(i, int) else None for i in shapes])\n",
    "\n",
    "\n",
    "def get_float_spec(tensor):\n",
    "    shape = get_shape_invariants(tensor)\n",
    "    return tf.TensorSpec(shape, dtype=tf.float32)\n",
    "\n",
    "class GLU(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        axis=-1,\n",
    "        name=\"glu_activation\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(GLU, self).__init__(name=name, **kwargs)\n",
    "        self.axis = axis\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        inputs,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        a, b = tf.split(inputs, 2, axis=self.axis)\n",
    "        b = tf.nn.sigmoid(b)\n",
    "        return tf.multiply(a, b)\n",
    "\n",
    "    def get_config(self):\n",
    "        conf = super(GLU, self).get_config()\n",
    "        conf.update({\"axis\": self.axis})\n",
    "        return conf\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads,\n",
    "        head_size,\n",
    "        output_size: int = None,\n",
    "        dropout: float = 0.0,\n",
    "        use_projection_bias: bool = True,\n",
    "        return_attn_coef: bool = False,\n",
    "        kernel_initializer: typing.Union[str, typing.Callable] = tf.keras.initializers.glorot_uniform(seed=SEED),\n",
    "        kernel_regularizer: typing.Union[str, typing.Callable] = None,\n",
    "        kernel_constraint: typing.Union[str, typing.Callable] = None,\n",
    "        bias_initializer: typing.Union[str, typing.Callable] = \"zeros\",\n",
    "        bias_regularizer: typing.Union[str, typing.Callable] = None,\n",
    "        bias_constraint: typing.Union[str, typing.Callable] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "\n",
    "        if output_size is not None and output_size < 1:\n",
    "            raise ValueError(\"output_size must be a positive number\")\n",
    "\n",
    "        self.kernel_initializer = tf.keras.initializers.get(kernel_initializer)\n",
    "        self.kernel_regularizer = tf.keras.regularizers.get(kernel_regularizer)\n",
    "        self.kernel_constraint = tf.keras.constraints.get(kernel_constraint)\n",
    "        self.bias_initializer = tf.keras.initializers.get(bias_initializer)\n",
    "        self.bias_regularizer = tf.keras.regularizers.get(bias_regularizer)\n",
    "        self.bias_constraint = tf.keras.constraints.get(bias_constraint)\n",
    "\n",
    "        self.head_size = head_size\n",
    "        self.num_heads = num_heads\n",
    "        self.output_size = output_size\n",
    "        self.use_projection_bias = use_projection_bias\n",
    "        self.return_attn_coef = return_attn_coef\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout, name=\"dropout\")\n",
    "        self._droput_rate = dropout\n",
    "        self.supports_masking = True # RAJOUT\n",
    "\n",
    "    def build(\n",
    "        self,\n",
    "        input_shape,\n",
    "    ):\n",
    "        num_query_features = input_shape[0][-1]\n",
    "        num_key_features = input_shape[1][-1]\n",
    "        num_value_features = input_shape[2][-1] if len(input_shape) > 2 else num_key_features\n",
    "        output_size = self.output_size if self.output_size is not None else num_value_features\n",
    "        self.query_kernel = self.add_weight(\n",
    "            name=\"query_kernel\",\n",
    "            shape=[self.num_heads, num_query_features, self.head_size],\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint,\n",
    "        )\n",
    "        self.key_kernel = self.add_weight(\n",
    "            name=\"key_kernel\",\n",
    "            shape=[self.num_heads, num_key_features, self.head_size],\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint,\n",
    "        )\n",
    "        self.value_kernel = self.add_weight(\n",
    "            name=\"value_kernel\",\n",
    "            shape=[self.num_heads, num_value_features, self.head_size],\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint,\n",
    "        )\n",
    "        self.projection_kernel = self.add_weight(\n",
    "            name=\"projection_kernel\",\n",
    "            shape=[self.num_heads, self.head_size, output_size],\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint,\n",
    "        )\n",
    "        if self.use_projection_bias:\n",
    "            self.projection_bias = self.add_weight(\n",
    "                name=\"projection_bias\",\n",
    "                shape=[output_size],\n",
    "                initializer=self.bias_initializer,\n",
    "                regularizer=self.bias_regularizer,\n",
    "                constraint=self.bias_constraint,\n",
    "            )\n",
    "        else:\n",
    "            self.projection_bias = None\n",
    "\n",
    "    def call_qkv(\n",
    "        self,\n",
    "        query,\n",
    "        key,\n",
    "        value,\n",
    "        training=None,\n",
    "    ):\n",
    "        # verify shapes\n",
    "        if key.shape[-2] != value.shape[-2]:\n",
    "            raise ValueError(\n",
    "                \"the number of elements in 'key' must be equal to \" \"the same as the number of elements in 'value'\"\n",
    "            )\n",
    "        # Linear transformations\n",
    "        query = tf.einsum(\"...NI,HIO->...NHO\", query, self.query_kernel)\n",
    "        key = tf.einsum(\"...MI,HIO->...MHO\", key, self.key_kernel)\n",
    "        value = tf.einsum(\"...MI,HIO->...MHO\", value, self.value_kernel)\n",
    "\n",
    "        return query, key, value\n",
    "\n",
    "    def call_attention(\n",
    "        self,\n",
    "        query,\n",
    "        key,\n",
    "        value,\n",
    "        logits,\n",
    "        training=None,\n",
    "        mask=None,\n",
    "        attention_mask=None,\n",
    "    ):\n",
    "        # mask = attention mask with shape [B, Tquery, Tkey] with 1 is for positions we want to attend, 0 for masked\n",
    "        if attention_mask is not None:\n",
    "            if len(attention_mask.shape) < 2: #was written mask\n",
    "                raise ValueError(\"'mask' must have at least 2 dimensions\")\n",
    "            if query.shape[-3] != attention_mask.shape[-2]:\n",
    "                raise ValueError(\"mask's second to last dimension must be equal to \" \"the number of elements in 'query'\")\n",
    "            if key.shape[-3] != attention_mask.shape[-1]:\n",
    "                raise ValueError(\"mask's last dimension must be equal to the number of elements in 'key'\")\n",
    "        # apply mask\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = tf.cast(attention_mask, tf.float32)\n",
    "\n",
    "            # possibly expand on the head dimension so broadcasting works\n",
    "            if len(attention_mask.shape) != len(logits.shape):\n",
    "                attention_mask = tf.expand_dims(attention_mask, -3)\n",
    "\n",
    "            logits += -10e9 * (1.0 - attention_mask)\n",
    "\n",
    "        attn_coef = tf.nn.softmax(logits)\n",
    "\n",
    "        # attention dropout\n",
    "        attn_coef_dropout = self.dropout(attn_coef, training=training)\n",
    "\n",
    "        # attention * value\n",
    "        multihead_output = tf.einsum(\"...HNM,...MHI->...NHI\", attn_coef_dropout, value)\n",
    "\n",
    "        # Run the outputs through another linear projection layer. Recombining heads\n",
    "        # is automatically done.\n",
    "        output = tf.einsum(\"...NHI,HIO->...NO\", multihead_output, self.projection_kernel)\n",
    "\n",
    "        if self.projection_bias is not None:\n",
    "            output += self.projection_bias\n",
    "\n",
    "        return output, attn_coef\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        inputs,\n",
    "        training=None,\n",
    "        mask=None,\n",
    "        attention_mask=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        query, key, value = inputs\n",
    "\n",
    "        query, key, value = self.call_qkv(query, key, value, training=training)\n",
    "\n",
    "        # Scale dot-product, doing the division to either query or key\n",
    "        # instead of their product saves some computation\n",
    "        depth = tf.constant(self.head_size, dtype=tf.float32)\n",
    "        query /= tf.sqrt(depth)\n",
    "\n",
    "        # Calculate dot product attention\n",
    "        logits = tf.einsum(\"...NHO,...MHO->...HNM\", query, key)\n",
    "\n",
    "        output, attn_coef = self.call_attention(query, key, value, logits, training=training, mask=mask, attention_mask=attention_mask)\n",
    "\n",
    "        if self.return_attn_coef:\n",
    "            return output, attn_coef\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def compute_output_shape(\n",
    "        self,\n",
    "        input_shape,\n",
    "    ):\n",
    "        num_value_features = input_shape[2][-1] if len(input_shape) > 2 else input_shape[1][-1]\n",
    "        output_size = self.output_size if self.output_size is not None else num_value_features\n",
    "\n",
    "        output_shape = input_shape[0][:-1] + (output_size,)\n",
    "\n",
    "        if self.return_attn_coef:\n",
    "            num_query_elements = input_shape[0][-2]\n",
    "            num_key_elements = input_shape[1][-2]\n",
    "            attn_coef_shape = input_shape[0][:-2] + (\n",
    "                self.num_heads,\n",
    "                num_query_elements,\n",
    "                num_key_elements,\n",
    "            )\n",
    "\n",
    "            return output_shape, attn_coef_shape\n",
    "        else:\n",
    "            return output_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "\n",
    "        config.update(\n",
    "            head_size=self.head_size,\n",
    "            num_heads=self.num_heads,\n",
    "            output_size=self.output_size,\n",
    "            dropout=self._droput_rate,\n",
    "            use_projection_bias=self.use_projection_bias,\n",
    "            return_attn_coef=self.return_attn_coef,\n",
    "            kernel_initializer=tf.keras.initializers.serialize(self.kernel_initializer),\n",
    "            kernel_regularizer=tf.keras.regularizers.serialize(self.kernel_regularizer),\n",
    "            kernel_constraint=tf.keras.constraints.serialize(self.kernel_constraint),\n",
    "            bias_initializer=tf.keras.initializers.serialize(self.bias_initializer),\n",
    "            bias_regularizer=tf.keras.regularizers.serialize(self.bias_regularizer),\n",
    "            bias_constraint=tf.keras.constraints.serialize(self.bias_constraint),\n",
    "        )\n",
    "\n",
    "        return config\n",
    "\n",
    "\n",
    "class RelPositionMultiHeadAttention(MultiHeadAttention):\n",
    "    def build(\n",
    "        self,\n",
    "        input_shape,\n",
    "    ):\n",
    "        num_pos_features = input_shape[-1][-1]\n",
    "        self.pos_kernel = self.add_weight(\n",
    "            name=\"pos_kernel\",\n",
    "            shape=[self.num_heads, num_pos_features, self.head_size],\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint,\n",
    "        )\n",
    "        self.pos_bias_u = self.add_weight(\n",
    "            name=\"pos_bias_u\",\n",
    "            shape=[self.num_heads, self.head_size],\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            initializer=self.kernel_initializer,\n",
    "            constraint=self.kernel_constraint,\n",
    "        )\n",
    "        self.pos_bias_v = self.add_weight(\n",
    "            name=\"pos_bias_v\",\n",
    "            shape=[self.num_heads, self.head_size],\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            initializer=self.kernel_initializer,\n",
    "            constraint=self.kernel_constraint,\n",
    "        )\n",
    "        super(RelPositionMultiHeadAttention, self).build(input_shape[:-1])\n",
    "\n",
    "    @staticmethod\n",
    "    def relative_shift(x):\n",
    "        x_shape = tf.shape(x)\n",
    "        x = tf.pad(x, [[0, 0], [0, 0], [0, 0], [1, 0]])\n",
    "        x = tf.reshape(x, [x_shape[0], x_shape[1], x_shape[3] + 1, x_shape[2]])\n",
    "        x = tf.reshape(x[:, :, 1:, :], x_shape)\n",
    "        return x\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        inputs,\n",
    "        training=None,\n",
    "        mask=None,\n",
    "        attention_mask=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        query, key, value, pos = inputs\n",
    "\n",
    "        query, key, value = self.call_qkv(query, key, value, training=training)\n",
    "\n",
    "        pos = tf.einsum(\"...MI,HIO->...MHO\", pos, self.pos_kernel)\n",
    "\n",
    "        query_with_u = query + self.pos_bias_u\n",
    "        query_with_v = query + self.pos_bias_v\n",
    "\n",
    "        logits_with_u = tf.einsum(\"...NHO,...MHO->...HNM\", query_with_u, key)\n",
    "        logits_with_v = tf.einsum(\"...NHO,...MHO->...HNM\", query_with_v, pos)\n",
    "        logits_with_v = self.relative_shift(logits_with_v)\n",
    "\n",
    "        logits = logits_with_u + logits_with_v[:, :, :, : tf.shape(logits_with_u)[3]]\n",
    "\n",
    "        depth = tf.constant(self.head_size, dtype=tf.float32)\n",
    "        logits /= tf.sqrt(depth)\n",
    "\n",
    "        output, attn_coef = self.call_attention(query, key, value, logits, training=training, mask=mask, attention_mask=attention_mask)\n",
    "\n",
    "        if self.return_attn_coef:\n",
    "            return output, attn_coef\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha: int = 1,\n",
    "        beta: int = 0,\n",
    "        name=\"positional_encoding\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(trainable=False, name=name, **kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.supports_masking = True # RAJOUT\n",
    "\n",
    "    def build(\n",
    "        self,\n",
    "        input_shape,\n",
    "    ):\n",
    "        dmodel = input_shape[-1]\n",
    "        assert dmodel % 2 == 0, f\"Input last dim must be even: {dmodel}\"\n",
    "\n",
    "    @staticmethod\n",
    "    def encode(\n",
    "        max_len,\n",
    "        dmodel,\n",
    "    ):\n",
    "        pos = tf.expand_dims(tf.range(max_len - 1, -1, -1.0, dtype=tf.float32), axis=1)\n",
    "        index = tf.expand_dims(tf.range(0, dmodel, dtype=tf.float32), axis=0)\n",
    "\n",
    "        pe = pos * (1 / tf.pow(10000.0, (2 * (index // 2)) / dmodel))\n",
    "\n",
    "        # Sin cos will be [max_len, size // 2]\n",
    "        # we add 0 between numbers by using padding and reshape\n",
    "        sin = tf.pad(tf.expand_dims(tf.sin(pe[:, 0::2]), -1), [[0, 0], [0, 0], [0, 1]], mode=\"CONSTANT\", constant_values=0)\n",
    "        sin = tf.reshape(sin, [max_len, dmodel])\n",
    "        cos = tf.pad(tf.expand_dims(tf.cos(pe[:, 1::2]), -1), [[0, 0], [0, 0], [1, 0]], mode=\"CONSTANT\", constant_values=0)\n",
    "        cos = tf.reshape(cos, [max_len, dmodel])\n",
    "        # Then add sin and cos, which results in [time, size]\n",
    "        pe = tf.add(sin, cos)\n",
    "        return tf.expand_dims(pe, axis=0)  # [1, time, size]\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        inputs,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # inputs shape [B, T, V]\n",
    "        _, max_len, dmodel = shape_list(inputs)\n",
    "        pe = self.encode(max_len * self.alpha + self.beta, dmodel)\n",
    "        return tf.cast(pe, dtype=inputs.dtype)\n",
    "\n",
    "    def get_config(self):\n",
    "        conf = super().get_config()\n",
    "        conf.update({\"alpha\": self.alpha, \"beta\": self.beta})\n",
    "        return conf\n",
    "\n",
    "\n",
    "class PositionalEncodingConcat(PositionalEncoding):\n",
    "    def build(\n",
    "        self,\n",
    "        input_shape,\n",
    "    ):\n",
    "        dmodel = input_shape[-1]\n",
    "        assert dmodel % 2 == 0, f\"Input last dim must be even: {dmodel}\"\n",
    "\n",
    "    @staticmethod\n",
    "    def encode(\n",
    "        max_len,\n",
    "        dmodel,\n",
    "    ):\n",
    "        pos = tf.range(max_len - 1, -1, -1.0, dtype=tf.float32)\n",
    "\n",
    "        index = tf.range(0, dmodel, 2.0, dtype=tf.float32)\n",
    "        index = 1 / tf.pow(10000.0, (index / dmodel))\n",
    "\n",
    "        sinusoid = tf.einsum(\"i,j->ij\", pos, index)\n",
    "        pos = tf.concat([tf.sin(sinusoid), tf.cos(sinusoid)], axis=-1)\n",
    "\n",
    "        return tf.expand_dims(pos, axis=0)\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        inputs,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # inputs shape [B, T, V]\n",
    "        _, max_len, dmodel = shape_list(inputs)\n",
    "        pe = self.encode(max_len * self.alpha + self.beta, dmodel)\n",
    "        return tf.cast(pe, dtype=inputs.dtype)\n",
    "\n",
    "\n",
    "\n",
    "L2 = tf.keras.regularizers.l2(1e-6)\n",
    "\n",
    "\n",
    "class FFModule(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        dropout=0.0,\n",
    "        fc_factor=0.5,\n",
    "        kernel_regularizer=L2,\n",
    "        bias_regularizer=L2,\n",
    "        name=\"ff_module\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(FFModule, self).__init__(name=name, **kwargs)\n",
    "        self.fc_factor = fc_factor\n",
    "        self.ln = tf.keras.layers.LayerNormalization(\n",
    "            name=f\"{name}_ln\",\n",
    "            gamma_regularizer=kernel_regularizer,\n",
    "            beta_regularizer=bias_regularizer,\n",
    "        )\n",
    "        self.ffn1 = tf.keras.layers.Dense(\n",
    "            4 * input_dim,\n",
    "            name=f\"{name}_dense_1\",\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "        )\n",
    "        self.swish = tf.keras.layers.Activation(tf.nn.swish, name=f\"{name}_swish_activation\")\n",
    "        self.do1 = tf.keras.layers.Dropout(dropout, name=f\"{name}_dropout_1\")\n",
    "        self.ffn2 = tf.keras.layers.Dense(\n",
    "            input_dim,\n",
    "            name=f\"{name}_dense_2\",\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "        )\n",
    "        self.do2 = tf.keras.layers.Dropout(dropout, name=f\"{name}_dropout_2\")\n",
    "        self.res_add = tfa.layers.StochasticDepth(name=f\"{name}_add\", survival_probability=0.4)\n",
    "        self.supports_masking = True # RAJOUT\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        inputs,\n",
    "        training=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        outputs = self.ln(inputs, training=training)\n",
    "        outputs = self.ffn1(outputs, training=training)\n",
    "        outputs = self.swish(outputs)\n",
    "        outputs = self.do1(outputs, training=training)\n",
    "        outputs = self.ffn2(outputs, training=training)\n",
    "        outputs = self.do2(outputs, training=training)\n",
    "        outputs = self.res_add([inputs, self.fc_factor * outputs])\n",
    "        return outputs\n",
    "\n",
    "    def get_config(self):\n",
    "        conf = super(FFModule, self).get_config()\n",
    "        conf.update({\"fc_factor\": self.fc_factor})\n",
    "        conf.update(self.ln.get_config())\n",
    "        conf.update(self.ffn1.get_config())\n",
    "        conf.update(self.swish.get_config())\n",
    "        conf.update(self.do1.get_config())\n",
    "        conf.update(self.ffn2.get_config())\n",
    "        conf.update(self.do2.get_config())\n",
    "        conf.update(self.res_add.get_config())\n",
    "        return conf\n",
    "\n",
    "\n",
    "class MHSAModule(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        head_size,\n",
    "        num_heads,\n",
    "        dropout=0.0,\n",
    "        mha_type=\"relmha\",\n",
    "        kernel_regularizer=L2,\n",
    "        bias_regularizer=L2,\n",
    "        name=\"mhsa_module\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(MHSAModule, self).__init__(name=name, **kwargs)\n",
    "        self.ln = tf.keras.layers.LayerNormalization(\n",
    "            name=f\"{name}_ln\",\n",
    "            gamma_regularizer=kernel_regularizer,\n",
    "            beta_regularizer=bias_regularizer,\n",
    "        )\n",
    "        if mha_type == \"relmha\":\n",
    "            self.mha = RelPositionMultiHeadAttention(\n",
    "                name=f\"{name}_mhsa\",\n",
    "                head_size=head_size,\n",
    "                num_heads=num_heads,\n",
    "                kernel_regularizer=kernel_regularizer,\n",
    "                bias_regularizer=bias_regularizer,\n",
    "            )\n",
    "        elif mha_type == \"mha\":\n",
    "            self.mha = MultiHeadAttention(\n",
    "                name=f\"{name}_mhsa\",\n",
    "                head_size=head_size,\n",
    "                num_heads=num_heads,\n",
    "                kernel_regularizer=kernel_regularizer,\n",
    "                bias_regularizer=bias_regularizer,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"mha_type must be either 'mha' or 'relmha'\")\n",
    "        self.do = tf.keras.layers.Dropout(dropout, name=f\"{name}_dropout\")\n",
    "        self.res_add = tfa.layers.StochasticDepth(name=f\"{name}_add\", survival_probability=0.5)\n",
    "        self.mha_type = mha_type\n",
    "        self.supports_masking = True # RAJOUT\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        inputs,\n",
    "        training=None,\n",
    "        mask=None,\n",
    "        attention_mask=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        inputs, pos = inputs  # pos is positional encoding\n",
    "        outputs = self.ln(inputs, training=training)\n",
    "        if self.mha_type == \"relmha\":\n",
    "            outputs = self.mha([outputs, outputs, outputs, pos], training=training, mask=mask, attention_mask=attention_mask)\n",
    "        else:\n",
    "            outputs = outputs + pos\n",
    "            outputs = self.mha([outputs, outputs, outputs], training=training, mask=mask, attention_mask=attention_mask)\n",
    "        outputs = self.do(outputs, training=training)\n",
    "        outputs = self.res_add([inputs, outputs])\n",
    "        return outputs\n",
    "\n",
    "    def get_config(self):\n",
    "        conf = super(MHSAModule, self).get_config()\n",
    "        conf.update({\"mha_type\": self.mha_type})\n",
    "        conf.update(self.ln.get_config())\n",
    "        conf.update(self.mha.get_config())\n",
    "        conf.update(self.do.get_config())\n",
    "        conf.update(self.res_add.get_config())\n",
    "        return conf\n",
    "\n",
    "\n",
    "class ConvModule(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        kernel_size=32,\n",
    "        dropout=0.0,\n",
    "        depth_multiplier=1,\n",
    "        kernel_regularizer=L2,\n",
    "        bias_regularizer=L2,\n",
    "        name=\"conv_module\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(ConvModule, self).__init__(name=name, **kwargs)\n",
    "        self.ln = tf.keras.layers.LayerNormalization()\n",
    "        self.pw_conv_1 = tf.keras.layers.Conv1D(\n",
    "            filters=2 * input_dim,\n",
    "            kernel_size=1,\n",
    "            strides=1,\n",
    "            padding=\"valid\",\n",
    "            name=f\"{name}_pw_conv_1\",\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "        )\n",
    "        self.glu = GLU(name=f\"{name}_glu\")\n",
    "        self.dw_conv = CausalDWConv1D(\n",
    "            kernel_size=kernel_size,\n",
    "            name=f\"{name}_dw_conv\",\n",
    "            depth_multiplier = depth_multiplier,\n",
    "        )\n",
    "        self.bn = tf.keras.layers.BatchNormalization(\n",
    "            name=f\"{name}_bn\",\n",
    "            gamma_regularizer=kernel_regularizer,\n",
    "            beta_regularizer=bias_regularizer,\n",
    "        )\n",
    "        self.swish = tf.keras.layers.Activation(\n",
    "            tf.nn.swish,\n",
    "            name=f\"{name}_swish_activation\",\n",
    "        )\n",
    "        self.pw_conv_2 = tf.keras.layers.Conv1D(\n",
    "            filters=input_dim,\n",
    "            kernel_size=1,\n",
    "            strides=1,\n",
    "            padding=\"valid\",\n",
    "            name=f\"{name}_pw_conv_2\",\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "        )\n",
    "        self.do = tf.keras.layers.Dropout(dropout, name=f\"{name}_dropout\")\n",
    "        self.res_add = tfa.layers.StochasticDepth(name=f\"{name}_add\", survival_probability=0.5)\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        inputs,\n",
    "        training=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        outputs = self.ln(inputs, training=training)\n",
    "        B, T, E = shape_list(outputs)\n",
    "        outputs = tf.reshape(outputs, [B, T, E]) # [B, T, 1, E]\n",
    "        outputs = self.pw_conv_1(outputs, training=training)\n",
    "        outputs = self.glu(outputs)\n",
    "        outputs = self.dw_conv(outputs, training=training)\n",
    "        outputs = self.bn(outputs, training=training)\n",
    "        outputs = self.swish(outputs)\n",
    "        outputs = self.pw_conv_2(outputs, training=training)\n",
    "        outputs = tf.reshape(outputs, [B, T, E]) #\n",
    "        outputs = self.do(outputs, training=training)\n",
    "        outputs = self.res_add([inputs, outputs])\n",
    "        return outputs\n",
    "\n",
    "    def get_config(self):\n",
    "        conf = super(ConvModule, self).get_config()\n",
    "        conf.update(self.ln.get_config())\n",
    "        conf.update(self.pw_conv_1.get_config())\n",
    "        conf.update(self.glu.get_config())\n",
    "        conf.update(self.dw_conv.get_config())\n",
    "        conf.update(self.bn.get_config())\n",
    "        conf.update(self.swish.get_config())\n",
    "        conf.update(self.pw_conv_2.get_config())\n",
    "        conf.update(self.do.get_config())\n",
    "        conf.update(self.res_add.get_config())\n",
    "        return conf\n",
    "\n",
    "\n",
    "\n",
    "class CausalDWConv1D(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "        kernel_size=17,\n",
    "        dilation_rate=1,\n",
    "        use_bias=False,\n",
    "        depthwise_initializer=tf.keras.initializers.glorot_uniform(seed=SEED),\n",
    "        depth_multiplier = 1,\n",
    "        name='', **kwargs):\n",
    "        super().__init__(name=name,**kwargs)\n",
    "        self.causal_pad = tf.keras.layers.ZeroPadding1D((dilation_rate*(kernel_size-1),0),name=name + '_pad')\n",
    "        self.dw_conv = tf.keras.layers.DepthwiseConv1D(\n",
    "                            kernel_size,\n",
    "                            strides=1,\n",
    "                            dilation_rate=dilation_rate,\n",
    "                            depth_multiplier=depth_multiplier,\n",
    "                            padding='valid',\n",
    "                            use_bias=use_bias,\n",
    "                            depthwise_initializer=depthwise_initializer,\n",
    "                            name=name + '_dwconv')\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.causal_pad(inputs)\n",
    "        x = self.dw_conv(x)\n",
    "        return x\n",
    "\n",
    "class ConformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        dropout=0.0,\n",
    "        fc_factor=0.5,\n",
    "        head_size=36,\n",
    "        num_heads=4,\n",
    "        mha_type=\"relmha\",\n",
    "        kernel_size=32,\n",
    "        depth_multiplier=1,\n",
    "        kernel_regularizer=L2,\n",
    "        bias_regularizer=L2,\n",
    "        name=\"conformer_block\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(ConformerBlock, self).__init__(name=name, **kwargs)\n",
    "        self.ffm1 = FFModule(\n",
    "            input_dim=input_dim,\n",
    "            dropout=dropout,\n",
    "            fc_factor=fc_factor,\n",
    "            name=f\"{name}_ff_module_1\",\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "        )\n",
    "        self.mhsam = MHSAModule(\n",
    "            mha_type=mha_type,\n",
    "            head_size=head_size,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name=f\"{name}_mhsa_module\",\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "        )\n",
    "        self.convm = ConvModule(\n",
    "            input_dim=input_dim,\n",
    "            kernel_size=kernel_size,\n",
    "            dropout=dropout,\n",
    "            name=f\"{name}_conv_module\",\n",
    "            depth_multiplier=depth_multiplier,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "        )\n",
    "        self.ffm2 = FFModule(\n",
    "            input_dim=input_dim,\n",
    "            dropout=dropout,\n",
    "            fc_factor=fc_factor,\n",
    "            name=f\"{name}_ff_module_2\",\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "        )\n",
    "        self.ln = tf.keras.layers.LayerNormalization(\n",
    "            name=f\"{name}_ln\",\n",
    "            gamma_regularizer=kernel_regularizer,\n",
    "            beta_regularizer=kernel_regularizer,\n",
    "        )\n",
    "        self.supports_masking = True # RAJOUT\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        inputs,\n",
    "        training=None,\n",
    "        mask=None,\n",
    "        attention_mask=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        inputs, pos = inputs  # pos is positional encoding\n",
    "        outputs = self.ffm1(inputs, training=training, **kwargs)\n",
    "        outputs = self.mhsam([outputs, pos], training=training, mask=mask, attention_mask=attention_mask, **kwargs)\n",
    "        outputs = self.convm(outputs, training=training, **kwargs)\n",
    "        outputs = self.ffm2(outputs, training=training, **kwargs)\n",
    "        outputs = self.ln(outputs, training=training)\n",
    "        return outputs\n",
    "\n",
    "    def get_config(self):\n",
    "        conf = super(ConformerBlock, self).get_config()\n",
    "        conf.update(self.ffm1.get_config())\n",
    "        conf.update(self.mhsam.get_config())\n",
    "        conf.update(self.convm.get_config())\n",
    "        conf.update(self.ffm2.get_config())\n",
    "        conf.update(self.ln.get_config())\n",
    "        return conf\n",
    "\n",
    "\n",
    "def get_attention_mask(x_inp, mask_value):\n",
    "    padding_mask = tf.reduce_sum(x_inp, axis=-1)\n",
    "    padding_mask = tf.cast(tf.math.equal(padding_mask, mask_value), tf.float32)\n",
    "    padding_mask = 1 - padding_mask\n",
    "    padding_mask = padding_mask[:, tf.newaxis, :]\n",
    "    return padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "092780ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T06:05:51.074281Z",
     "iopub.status.busy": "2023-08-25T06:05:51.074013Z",
     "iopub.status.idle": "2023-08-25T06:05:51.080676Z",
     "shell.execute_reply": "2023-08-25T06:05:51.079693Z"
    },
    "papermill": {
     "duration": 0.020655,
     "end_time": "2023-08-25T06:05:51.082891",
     "exception": false,
     "start_time": "2023-08-25T06:05:51.062236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CTCLoss(labels, logits):\n",
    "    label_length = tf.reduce_sum(tf.cast(labels != pad_token_idx, tf.int32), axis=-1)\n",
    "    logit_length = tf.ones(tf.shape(logits)[0], dtype=tf.int32) * tf.shape(logits)[1]\n",
    "    \n",
    "    loss = classic_ctc_loss(\n",
    "            labels=labels,\n",
    "            logits=logits,\n",
    "            label_length=label_length,\n",
    "            logit_length=logit_length,\n",
    "            blank_index=pad_token_idx,\n",
    "        )\n",
    "    '''\n",
    "    loss = tf.nn.ctc_loss(\n",
    "            labels=labels,\n",
    "            logits=logits,\n",
    "            label_length=label_length,\n",
    "            logit_length=logit_length,\n",
    "            blank_index=pad_token_idx,\n",
    "            logits_time_major=False\n",
    "        )\n",
    "    '''\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfc57872",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T06:05:51.107838Z",
     "iopub.status.busy": "2023-08-25T06:05:51.107560Z",
     "iopub.status.idle": "2023-08-25T06:05:51.113057Z",
     "shell.execute_reply": "2023-08-25T06:05:51.112037Z"
    },
    "papermill": {
     "duration": 0.019159,
     "end_time": "2023-08-25T06:05:51.115162",
     "exception": false,
     "start_time": "2023-08-25T06:05:51.096003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Config param\n",
    "\n",
    "INPUT_SHAPE = [128, 276] #format d'entrée\n",
    "dim = 256 # Embedding dimension\n",
    "num_blocs = 12\n",
    "dropout_cformer = 0.0 # Dropout applied in each module of conformer block, in different location in modules\n",
    "num_heads = 8\n",
    "head_size = dim // num_heads # head_size * num_heads should be equal to dim\n",
    "depth_multiplier = 1 # didn't try to modify it but I think it will compile anyway\n",
    "kernel_size = 3 # kernel size of Conv module, specially the dephwiseConv1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7543ec0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T06:05:51.137976Z",
     "iopub.status.busy": "2023-08-25T06:05:51.137701Z",
     "iopub.status.idle": "2023-08-25T06:05:51.147595Z",
     "shell.execute_reply": "2023-08-25T06:05:51.146751Z"
    },
    "papermill": {
     "duration": 0.023723,
     "end_time": "2023-08-25T06:05:51.149764",
     "exception": false,
     "start_time": "2023-08-25T06:05:51.126041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "\n",
    "def get_model(dim = dim, num_blocs = num_blocs):\n",
    "    inp = tf.keras.Input(INPUT_SHAPE, name=\"input\")\n",
    "\n",
    "#         attention_mask = get_attention_mask(inp, value_pad=-100.0)\n",
    "#         mask = tf.keras.layers.Masking(mask_value=-100.0, input_shape=INPUT_SHAPE)\n",
    "\n",
    "#         x = mask(inp)\n",
    "    x = tf.keras.layers.Dense(dim, use_bias=False,name='stem_conv')(inp)\n",
    "    pe = PositionalEncoding()\n",
    "    pex = pe(x)\n",
    "        \n",
    "   # x = SpatialDropout1D(0.2, name='spatial_dropout_pe')(x)  # DO spatial\n",
    "\n",
    "    # x = tf.keras.layers.Dropout(0.4)(x) # you can activate that sku sku dropout if you want to\n",
    "\n",
    "    conf_blocks = []\n",
    "    for i in range(num_blocs):\n",
    "        name = f'Conf_block_{i}'\n",
    "        conf_block= ConformerBlock(input_dim=dim,\n",
    "                                   head_size=head_size,\n",
    "                                   dropout=dropout_cformer,\n",
    "                                   num_heads=num_heads,\n",
    "                                   depth_multiplier=depth_multiplier,\n",
    "                                   kernel_size=kernel_size,\n",
    "                                   name=name)\n",
    "        conf_blocks.append(conf_block)\n",
    "\n",
    "    for cblock in conf_blocks:\n",
    "        x = cblock([x, pex])\n",
    "\n",
    "   # x = SpatialDropout1D(0.2, name='spatial_dropout_pe')(x)  # DO spatial\n",
    "    x = tf.keras.layers.Dense(dim*2,activation=\"relu\",name='top_conv')(x)\n",
    "    x = tf.keras.layers.Dropout(0.7)(x)\n",
    "    x = tf.keras.layers.Dense(len(char_to_num))(x)\n",
    "\n",
    "    model = tf.keras.Model(inp, x)\n",
    "\n",
    "    loss = CTCLoss\n",
    "\n",
    "    #Adam Optimizer\n",
    "    optimizer = tfa.optimizers.RectifiedAdam(sma_threshold=4)\n",
    "    optimizer = tfa.optimizers.Lookahead(optimizer, sync_period=5)\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07352109",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T06:05:51.172284Z",
     "iopub.status.busy": "2023-08-25T06:05:51.172024Z",
     "iopub.status.idle": "2023-08-25T06:05:51.180633Z",
     "shell.execute_reply": "2023-08-25T06:05:51.179787Z"
    },
    "papermill": {
     "duration": 0.022722,
     "end_time": "2023-08-25T06:05:51.183070",
     "exception": false,
     "start_time": "2023-08-25T06:05:51.160348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def num_to_char_fn(y):\n",
    "    return [num_to_char.get(x, \"\") for x in y]\n",
    "\n",
    "@tf.function()\n",
    "def decode_phrase(pred):\n",
    "    x = tf.argmax(pred, axis=1)\n",
    "    diff = tf.not_equal(x[:-1], x[1:])\n",
    "    adjacent_indices = tf.where(diff)[:, 0]\n",
    "    x = tf.gather(x, adjacent_indices)\n",
    "    mask = x != pad_token_idx\n",
    "    x = tf.boolean_mask(x, mask, axis=0)\n",
    "    return x\n",
    "\n",
    "# A utility function to decode the output of the network\n",
    "def decode_batch_predictions(pred):\n",
    "    output_text = []\n",
    "    for result in pred:\n",
    "        result = \"\".join(num_to_char_fn(decode_phrase(result).numpy()))\n",
    "        output_text.append(result)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0190837",
   "metadata": {
    "papermill": {
     "duration": 0.010507,
     "end_time": "2023-08-25T06:05:51.203876",
     "exception": false,
     "start_time": "2023-08-25T06:05:51.193369",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8191115d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T06:05:51.225689Z",
     "iopub.status.busy": "2023-08-25T06:05:51.225436Z",
     "iopub.status.idle": "2023-08-25T06:05:57.519975Z",
     "shell.execute_reply": "2023-08-25T06:05:57.518993Z"
    },
    "papermill": {
     "duration": 6.307827,
     "end_time": "2023-08-25T06:05:57.522125",
     "exception": false,
     "start_time": "2023-08-25T06:05:51.214298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 128, 276)]   0           []                               \n",
      "                                                                                                  \n",
      " stem_conv (Dense)              (None, 128, 256)     70656       ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " positional_encoding (Positiona  (1, 128, 256)       0           ['stem_conv[0][0]']              \n",
      " lEncoding)                                                                                       \n",
      "                                                                                                  \n",
      " Conf_block_0 (ConformerBlock)  (None, 128, 256)     1581312     ['stem_conv[0][0]',              \n",
      "                                                                  'positional_encoding[0][0]']    \n",
      "                                                                                                  \n",
      " Conf_block_1 (ConformerBlock)  (None, 128, 256)     1581312     ['Conf_block_0[0][0]',           \n",
      "                                                                  'positional_encoding[0][0]']    \n",
      "                                                                                                  \n",
      " Conf_block_2 (ConformerBlock)  (None, 128, 256)     1581312     ['Conf_block_1[0][0]',           \n",
      "                                                                  'positional_encoding[0][0]']    \n",
      "                                                                                                  \n",
      " Conf_block_3 (ConformerBlock)  (None, 128, 256)     1581312     ['Conf_block_2[0][0]',           \n",
      "                                                                  'positional_encoding[0][0]']    \n",
      "                                                                                                  \n",
      " Conf_block_4 (ConformerBlock)  (None, 128, 256)     1581312     ['Conf_block_3[0][0]',           \n",
      "                                                                  'positional_encoding[0][0]']    \n",
      "                                                                                                  \n",
      " Conf_block_5 (ConformerBlock)  (None, 128, 256)     1581312     ['Conf_block_4[0][0]',           \n",
      "                                                                  'positional_encoding[0][0]']    \n",
      "                                                                                                  \n",
      " Conf_block_6 (ConformerBlock)  (None, 128, 256)     1581312     ['Conf_block_5[0][0]',           \n",
      "                                                                  'positional_encoding[0][0]']    \n",
      "                                                                                                  \n",
      " Conf_block_7 (ConformerBlock)  (None, 128, 256)     1581312     ['Conf_block_6[0][0]',           \n",
      "                                                                  'positional_encoding[0][0]']    \n",
      "                                                                                                  \n",
      " Conf_block_8 (ConformerBlock)  (None, 128, 256)     1581312     ['Conf_block_7[0][0]',           \n",
      "                                                                  'positional_encoding[0][0]']    \n",
      "                                                                                                  \n",
      " Conf_block_9 (ConformerBlock)  (None, 128, 256)     1581312     ['Conf_block_8[0][0]',           \n",
      "                                                                  'positional_encoding[0][0]']    \n",
      "                                                                                                  \n",
      " Conf_block_10 (ConformerBlock)  (None, 128, 256)    1581312     ['Conf_block_9[0][0]',           \n",
      "                                                                  'positional_encoding[0][0]']    \n",
      "                                                                                                  \n",
      " Conf_block_11 (ConformerBlock)  (None, 128, 256)    1581312     ['Conf_block_10[0][0]',          \n",
      "                                                                  'positional_encoding[0][0]']    \n",
      "                                                                                                  \n",
      " top_conv (Dense)               (None, 128, 512)     131584      ['Conf_block_11[0][0]']          \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 128, 512)     0           ['top_conv[0][0]']               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128, 60)      30780       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 19,208,764\n",
      "Trainable params: 19,202,620\n",
      "Non-trainable params: 6,144\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Clear all models in GPU\n",
    "tf.keras.backend.clear_session()\n",
    "seed_everything(seed=SEED)\n",
    "# Get new fresh model\n",
    "model = get_model()\n",
    "# model(batch[0])\n",
    "# Sanity Check\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83542487",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T06:05:57.554417Z",
     "iopub.status.busy": "2023-08-25T06:05:57.554125Z",
     "iopub.status.idle": "2023-08-25T06:05:58.945020Z",
     "shell.execute_reply": "2023-08-25T06:05:58.944040Z"
    },
    "papermill": {
     "duration": 1.409471,
     "end_time": "2023-08-25T06:05:58.947383",
     "exception": false,
     "start_time": "2023-08-25T06:05:57.537912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Loaded Pretrained Weights\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('/kaggle/input/zzzzzz/model_epoch_132_val_loss_17.1502.h5')\n",
    "print(f'Successfully Loaded Pretrained Weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "017f687f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T06:05:58.979392Z",
     "iopub.status.busy": "2023-08-25T06:05:58.979073Z",
     "iopub.status.idle": "2023-08-25T06:05:58.984277Z",
     "shell.execute_reply": "2023-08-25T06:05:58.983382Z"
    },
    "papermill": {
     "duration": 0.023796,
     "end_time": "2023-08-25T06:05:58.986932",
     "exception": false,
     "start_time": "2023-08-25T06:05:58.963136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "stem_conv\n",
      "positional_encoding\n",
      "Conf_block_0\n",
      "Conf_block_1\n",
      "Conf_block_2\n",
      "Conf_block_3\n",
      "Conf_block_4\n",
      "Conf_block_5\n",
      "Conf_block_6\n",
      "Conf_block_7\n",
      "Conf_block_8\n",
      "Conf_block_9\n",
      "Conf_block_10\n",
      "Conf_block_11\n",
      "top_conv\n",
      "dropout\n",
      "dense\n"
     ]
    }
   ],
   "source": [
    "#model.get_weight_paths()\n",
    "# Model Layer Names\n",
    "for l in model.layers:\n",
    "    print(l.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c88d9c21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T06:05:59.019090Z",
     "iopub.status.busy": "2023-08-25T06:05:59.018817Z",
     "iopub.status.idle": "2023-08-25T06:06:06.765537Z",
     "shell.execute_reply": "2023-08-25T06:06:06.764512Z"
    },
    "papermill": {
     "duration": 7.765181,
     "end_time": "2023-08-25T06:06:06.767658",
     "exception": false,
     "start_time": "2023-08-25T06:05:59.002477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([13, 59])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TFLiteModel(tf.Module):\n",
    "    def __init__(self, model):\n",
    "        super(TFLiteModel, self).__init__()\n",
    "        self.model = model\n",
    "    \n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, len(SEL_COLS)], dtype=tf.float32, name='inputs')])\n",
    "    def __call__(self, inputs, training=False):\n",
    "        # Preprocess Data\n",
    "        x = tf.cast(inputs, tf.float32)\n",
    "        x = x[None]\n",
    "        x = tf.cond(tf.shape(x)[1] == 0, lambda: tf.zeros((1, 1, len(SEL_COLS))), lambda: tf.identity(x))\n",
    "        x = x[0]\n",
    "        x = pre_process0(x)\n",
    "        x = pre_process1(*x)\n",
    "        x = tf.reshape(x, INPUT_SHAPE)\n",
    "        x = x[None]\n",
    "        x = self.model(x, training=False)\n",
    "        x = x[0]\n",
    "        x = decode_phrase(x)\n",
    "        x = tf.cond(tf.shape(x)[0] == 0, lambda: tf.zeros(1, tf.int64), lambda: tf.identity(x))\n",
    "        x = tf.one_hot(x, 59)\n",
    "        return {'outputs': x}\n",
    "\n",
    "tflitemodel_base = TFLiteModel(model)\n",
    "tflitemodel_base(frames)[\"outputs\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b4a8bce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T06:06:06.801406Z",
     "iopub.status.busy": "2023-08-25T06:06:06.800610Z",
     "iopub.status.idle": "2023-08-25T06:08:56.486837Z",
     "shell.execute_reply": "2023-08-25T06:08:56.485834Z"
    },
    "papermill": {
     "duration": 169.705641,
     "end_time": "2023-08-25T06:08:56.489669",
     "exception": false,
     "start_time": "2023-08-25T06:06:06.784028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "keras_model_converter = tf.lite.TFLiteConverter.from_keras_model(tflitemodel_base)\n",
    "keras_model_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]#, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "keras_model_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "keras_model_converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_model = keras_model_converter.convert()\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56e4d7aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T06:08:56.523396Z",
     "iopub.status.busy": "2023-08-25T06:08:56.523056Z",
     "iopub.status.idle": "2023-08-25T06:08:56.529209Z",
     "shell.execute_reply": "2023-08-25T06:08:56.527775Z"
    },
    "papermill": {
     "duration": 0.025711,
     "end_time": "2023-08-25T06:08:56.531587",
     "exception": false,
     "start_time": "2023-08-25T06:08:56.505876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add selected_columns json to only select specific columns from input frames\n",
    "with open('inference_args.json', \"w\") as f:\n",
    "    json.dump({\"selected_columns\" : SEL_COLS}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28d1ac46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T06:08:56.563202Z",
     "iopub.status.busy": "2023-08-25T06:08:56.562919Z",
     "iopub.status.idle": "2023-08-25T06:08:59.999373Z",
     "shell.execute_reply": "2023-08-25T06:08:59.998132Z"
    },
    "papermill": {
     "duration": 3.45514,
     "end_time": "2023-08-25T06:09:00.001961",
     "exception": false,
     "start_time": "2023-08-25T06:08:56.546821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: model.tflite (deflated 10%)\r\n",
      "  adding: inference_args.json (deflated 83%)\r\n"
     ]
    }
   ],
   "source": [
    "# Zip Model\n",
    "!zip submission.zip  './model.tflite' './inference_args.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69939b7",
   "metadata": {
    "papermill": {
     "duration": 0.015421,
     "end_time": "2023-08-25T06:09:00.035870",
     "exception": false,
     "start_time": "2023-08-25T06:09:00.020449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 221.881114,
   "end_time": "2023-08-25T06:09:03.173509",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-08-25T06:05:21.292395",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
